{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d97f8e6",
   "metadata": {},
   "source": [
    "# Agent Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614cd287",
   "metadata": {},
   "source": [
    "## Part 1: Setup all our imports\n",
    "\n",
    "We have already installed all the needed pip packages needed to run this code.  For reference, they included:\n",
    "\n",
    "Needed for core code:\n",
    "- psycopg-pool\n",
    "- psycopg-binary\n",
    "- psycopg2-binary\n",
    "- pydantic\n",
    "- openai\n",
    "- semantic-kernel\n",
    "\n",
    "Needed for Notebook implementation:\n",
    "- ipykernel\n",
    "- nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import psycopg2\n",
    "import uuid\n",
    "import requests\n",
    "from typing import Annotated\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.functions import kernel_function, KernelArguments\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.memory.postgres import PostgresMemoryStore\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding import AzureTextEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194337c",
   "metadata": {},
   "source": [
    "# Part 2: Setup environmental connection variables\n",
    "\n",
    "Update the variables and DB_CONFIG object below, by running the script located in /Scripts/get_env.ps1\n",
    "\n",
    "Open a new terminal and at the following path, enter:\n",
    "\n",
    "`PS C:\\Users\\LabUser\\Downloads\\pg-sk-agents-lab> .\\Scripts\\get_env.ps1`\n",
    "\n",
    "Copy the values for each into the variables in the code block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56021df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_ENDPOINT   = \"\"\n",
    "AZURE_OPENAI_KEY        = \"\"\n",
    "AZURE_OPENAI_DEPLOYMENT = \"gpt-4o\"\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\":     \"\",\n",
    "    \"dbname\":   \"cases\",\n",
    "    \"user\":     \"\",\n",
    "    \"password\": \"\",\n",
    "    \"port\":     5432,\n",
    "    \"sslmode\":  \"require\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a3fd1",
   "metadata": {},
   "source": [
    "# Part 3: Create Semantic Kernel Plugin for Basic Database Queries\n",
    "\n",
    "In this step we are going to add some initial Database Search functions in this PlugIn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseSearchPlugin:\n",
    "        def __init__(self, cfg):\n",
    "            self.cfg = cfg\n",
    "\n",
    "        @kernel_function(description=\"Return the total number of cases in the database.\")\n",
    "        def count_cases(self) -> str:\n",
    "            \n",
    "            print(\"count_cases was called\")\n",
    "            \n",
    "            conn = psycopg2.connect(**self.cfg)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"SELECT COUNT(*) FROM cases;\")\n",
    "            n = cur.fetchone()[0]\n",
    "            conn.close()\n",
    "            return str(n)\n",
    "\n",
    "        @kernel_function(description=\"Find up to 10 case IDs and names whose opinion contains the given keyword.\")\n",
    "        def search_cases(self, keyword: str) -> str:\n",
    "            \n",
    "            print(\"search_cases was called\")\n",
    "            \n",
    "            conn = psycopg2.connect(**self.cfg)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\n",
    "                \"SELECT id, name, opinion FROM cases WHERE opinion ILIKE %s LIMIT 10;\",\n",
    "                (f\"%{keyword}%\",)\n",
    "            )\n",
    "            rows = cur.fetchall()\n",
    "            conn.close()\n",
    "            if not rows:\n",
    "                return \"No matches\"\n",
    "            return \"\\n\".join(f\"{r[0]}: {r[1]}\" for r in rows)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1206c174",
   "metadata": {},
   "source": [
    "# Add a Semantic Re-ranking Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef674482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRerankingPlugin:\n",
    "        def __init__(self, cfg):\n",
    "            self.cfg = cfg\n",
    "\n",
    "        @kernel_function(description=\"Use semantic re-ranking function to query and find cases matching the query based on semantic intent and relevance.  Use this function when high accuracy is needed.\")\n",
    "        def search_semantic_reranked_cases(self, query: str) -> str:\n",
    "            \n",
    "            print(\"search_semantic_reranked_cases was called\")\n",
    "            \n",
    "            conn = psycopg2.connect(**self.cfg)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                WITH embedding_query AS (\n",
    "                    SELECT azure_openai.create_embeddings('text-embedding-3-small', %s)::vector AS embedding\n",
    "                ),\n",
    "                vector AS (\n",
    "                    SELECT cases.id as case_id, cases.name AS case_name, cases.opinion, RANK() OVER (ORDER BY opinions_vector <=> embedding) AS vector_rank\n",
    "                    FROM cases, embedding_query\n",
    "                    ORDER BY opinions_vector <=> embedding\n",
    "                    LIMIT 60\n",
    "                ),\n",
    "                semantic AS (\n",
    "                    SELECT * \n",
    "                    FROM jsonb_array_elements(\n",
    "                            semantic_relevance(%s, 60)\n",
    "                        ) WITH ORDINALITY AS elem(relevance)\n",
    "                ),\n",
    "                semantic_ranked AS (\n",
    "                    SELECT semantic.relevance::DOUBLE PRECISION AS relevance, RANK() OVER (ORDER BY relevance DESC) AS semantic_rank,\n",
    "                            semantic.*, vector.*\n",
    "                    FROM vector\n",
    "                    JOIN semantic ON vector.vector_rank = semantic.ordinality\n",
    "                    ORDER BY semantic.relevance DESC\n",
    "                )\n",
    "                SELECT case_id, case_name, opinion\n",
    "                FROM semantic_ranked\n",
    "                LIMIT 10;\n",
    "                \"\"\", (query, query))\n",
    "            rows = cur.fetchall()\n",
    "            conn.close()\n",
    "            if not rows:\n",
    "                return \"No matches\"\n",
    "            return \"\\n\".join(f\"{r[0]}: {r[1]}: {r[2][:1000]}\" for r in rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1491f",
   "metadata": {},
   "source": [
    "# Add a GraphRAG Query PlugIn to the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ec57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDatabasePlugin:\n",
    "        def __init__(self, cfg):\n",
    "            self.cfg = cfg\n",
    "\n",
    "        @kernel_function(description=\"Use an advanced accuracy query to find important cases with high levels of citations about the query topic.\")\n",
    "        def search_graph_cases(self, query: str) -> str:\n",
    "            \n",
    "            print(\"search_graph_cases was called\")\n",
    "            \n",
    "            conn = psycopg2.connect(**self.cfg)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                SET search_path = public, ag_catalog, \"$user\";\n",
    "\n",
    "                WITH semantic_ranked AS (\n",
    "                    SELECT id, name, opinion, opinions_vector\n",
    "                    FROM cases\n",
    "                    ORDER BY opinions_vector <=> azure_openai.create_embeddings('text-embedding-3-small', %s)::vector\n",
    "                    LIMIT 60\n",
    "                ),\n",
    "                graph AS (\n",
    "                    SELECT graph_query.refs, semantic_ranked.*, graph_query.case_id \n",
    "                    FROM semantic_ranked\n",
    "                    LEFT JOIN cypher('case_graph', $$\n",
    "                        MATCH ()-[r]->(n)\n",
    "                        RETURN n.case_id, COUNT(r) AS refs\n",
    "                    $$) as graph_query(case_id TEXT, refs BIGINT)\n",
    "                    ON semantic_ranked.id = graph_query.case_id::int\n",
    "                )\n",
    "                SELECT id, name, opinion\n",
    "                FROM graph\n",
    "                ORDER BY refs DESC NULLS LAST\n",
    "                LIMIT 10;\n",
    "                \"\"\", \n",
    "                (f\"%{query}%\",)\n",
    "            )\n",
    "            rows = cur.fetchall()\n",
    "            conn.close()\n",
    "            if not rows:\n",
    "                return \"No matches\"\n",
    "            return \"\\n\".join(f\"{r[0]}: {r[1]}: {r[2][:1000]}\" for r in rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32517a",
   "metadata": {},
   "source": [
    "# Adding a Weather PlugIn to the Agent\n",
    "\n",
    "Since we are building a real estate law agent, having evidence of historic weather would be helpful.\n",
    "This Weather Plugin contains a function to reach out into the Internet to search for past rainfall.\n",
    "When a user prompt mentions past rainfall, this Agent Tool will get called to give accurate, grounded context to the final prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9190f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherPlugin:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Get total precipitation (in mm) on a given date and location (latitude, longitude).\"\n",
    "    )\n",
    "    def get_historical_rainfall(self, date: str, latitude: float, longitude: float) -> str:\n",
    "        \"\"\"\n",
    "        date: YYYY-MM-DD\n",
    "        latitude, longitude: WGS84 coords\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"get_historical_rainfall was called\")\n",
    "\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"start_date\": date,\n",
    "            \"end_date\":   date,\n",
    "            \"daily\":      \"precipitation_sum\",\n",
    "            \"timezone\":   \"UTC\"\n",
    "        }\n",
    "        resp = requests.get(url, params=params)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        data = resp.json()\n",
    "        # the API returns a list for daily.precipitation_sum\n",
    "        rain_mm = data[\"daily\"][\"precipitation_sum\"][0]\n",
    "        return f\"On {date} at ({latitude}, {longitude}), total precipitation was {rain_mm} mm.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b672c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = OpenAIChatPromptExecutionSettings()\n",
    "    \n",
    "\n",
    "chat_svc = AzureChatCompletion(\n",
    "    deployment_name=AZURE_OPENAI_DEPLOYMENT,\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_KEY)\n",
    "\n",
    "# Create agent with plugin and settings\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_svc,\n",
    "    name=\"SK-Assistant\",\n",
    "    instructions=\"You are a helpful legal assistant.  Respond to the user with the name of the cases and reasoning why the cases are the most relevant, and a short sentence summary of the opinion of the cases.\",\n",
    "    plugins=[DatabaseSearchPlugin(DB_CONFIG),SemanticRerankingPlugin(DB_CONFIG),GraphDatabasePlugin(DB_CONFIG),WeatherPlugin()],\n",
    "    arguments=KernelArguments(settings)\n",
    ")\n",
    "\n",
    "#user_query = \"Find me some cases regarding the notion of my house falling down.\"\n",
    "#user_query = \"I am a real estate lawyer, so cases need to be related to real estate law.\"\n",
    "#user_query = \"Find me 10 cases regarding the notion of water leaking.\"\n",
    "#user_query = \"How many cases are there, and high accuracy is important, help me find 10 highly relevant cases related to water leaking in my apartment.\"\n",
    "#user_query = \"Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.  Also how many cases are there overall?\"\n",
    "#user_query = \"Bring into 1 list of 10 cases, ranked by relevancy -- Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.\"\n",
    "\n",
    "#user_query = \"How many cases are in my database? What was the rainfall on 2024-02-01 in Seattle, WA? I need this for evidence.  Also High accuracy is important, help me find 10 highly relevant cases related to water leaking in my clients apartment.\"\n",
    "\n",
    "user_query = \"What was the rainfall on 2024-02-01 in Seattle, WA? I need this for evidence.  Also High accuracy is important, help me find 10 highly relevant cases related to water leaking in my clients apartment.\"\n",
    "\n",
    "response = await agent.get_response(messages=user_query)\n",
    "\n",
    "print(\"**response.content**\")\n",
    "print(response.content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784b47b",
   "metadata": {},
   "source": [
    "# Adding Memory into the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f914df",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = OpenAIChatPromptExecutionSettings()\n",
    "    \n",
    "\n",
    "chat_svc = AzureChatCompletion(\n",
    "    deployment_name=AZURE_OPENAI_DEPLOYMENT,\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_KEY)\n",
    "\n",
    "# Create agent with plugin and settings\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_svc,\n",
    "    name=\"SK-Assistant\",\n",
    "    instructions=\"You are a helpful legal assistant.  Respond to the user with the name of the cases and reasoning why the cases are the most relevant, and a short sentence summary of the opinion of the cases.\",\n",
    "    plugins=[DatabaseSearchPlugin(DB_CONFIG),SemanticRerankingPlugin(DB_CONFIG),GraphDatabasePlugin(DB_CONFIG),WeatherPlugin()],\n",
    "    arguments=KernelArguments(settings)\n",
    ")\n",
    "\n",
    "conn_str = (\n",
    "f\"host={DB_CONFIG['host']} \"\n",
    "f\"port={DB_CONFIG['port']} \"\n",
    "f\"dbname={DB_CONFIG['dbname']} \"\n",
    "f\"user={DB_CONFIG['user']} \"\n",
    "f\"password={DB_CONFIG['password']} \"\n",
    "f\"sslmode={DB_CONFIG['sslmode']}\"\n",
    ")\n",
    "\n",
    "memory_store = PostgresMemoryStore(\n",
    "    connection_string=conn_str,\n",
    "    default_dimensionality=1536\n",
    ")\n",
    "\n",
    "embedding_generator = AzureTextEmbedding(\n",
    "    deployment_name=\"text-embedding-3-small\",\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_KEY\n",
    ")\n",
    "\n",
    "semantic_memory = SemanticTextMemory(\n",
    "    storage=memory_store,\n",
    "    embeddings_generator=embedding_generator\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#user_query = \"Find me some cases regarding the notion of my house falling down.\"\n",
    "#user_query = \"I am an environmental lawyer, so cases need to be related to environmental law.\"\n",
    "#user_query = \"Find me 10 cases regarding the notion of water leaking.\"\n",
    "#user_query = \"How many cases are there, and high accuracy is important, help me find 10 highly relevant cases related to water leaking in my apartment.\"\n",
    "#user_query = \"Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.  Also how many cases are there overall?\"\n",
    "user_query = \"Bring into 1 list of 10 cases, ranked by relevancy -- Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the user query into memory\n",
    "await semantic_memory.save_information(\n",
    "    collection=\"agent_memories\",\n",
    "    text=user_query,\n",
    "    id=str(uuid.uuid4()),\n",
    "    description=\"User query\"\n",
    ")\n",
    "\n",
    "# Retrieve top-3 related memories for context\n",
    "recalls = await semantic_memory.search(\n",
    "    collection=\"agent_memories\",\n",
    "    query=user_query,\n",
    "    limit=3\n",
    ")\n",
    "# Build a little bullet list\n",
    "memory_context = \"\\n\".join(f\"- {m.text}\" for m in recalls)\n",
    "\n",
    "# Prepend memory to the prompt\n",
    "prompt = (\n",
    "    f\"Here are things we’ve discussed before:\\n{memory_context}\\n\\n\"\n",
    "    f\"{user_query}\"\n",
    ")\n",
    "\n",
    "\n",
    "response = await agent.get_response(messages=prompt)\n",
    "\n",
    "print(\"prompt with memory context\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"response.content\")\n",
    "print(response.content)\n",
    "\n",
    "\n",
    "\n",
    "# Save the agent’s reply back to memory\n",
    "await semantic_memory.save_information(\n",
    "    collection=\"agent_memories\",\n",
    "    text=str(response.content),\n",
    "    id=str(uuid.uuid4()),\n",
    "    description=\"Agent reply\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
