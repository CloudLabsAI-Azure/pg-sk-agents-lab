{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d97f8e6",
   "metadata": {},
   "source": [
    "## Build an Agentic App with PostgreSQL, GraphRAG, and Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e253c9",
   "metadata": {},
   "source": [
    "### Part 3.1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7fafd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "In this notebook, you will build a Semantic Kernel Agent that can reason over our database of legal cases you deployed in the previous steps. You will also incorporate external web service data, and use memory to improve its responses over time.\n",
    "\n",
    "Semantic Kernel is an open-source SDK developed by Microsoft that helps developers create advanced AI agents by combining:\n",
    "\n",
    "- LLMs (Large Language Models) like OpenAI's GPT models\n",
    "- Plugins (custom tools and functions the agent can call)\n",
    "- Memory (saving and recalling past conversations or facts)\n",
    "\n",
    "An Agent in Semantic Kernel is a smart assistant that can:\n",
    "\n",
    "- Respond to user prompts\n",
    "- Decide which plugin functions to call\n",
    "- Use external knowledge sources like databases or APIs\n",
    "- Build better, grounded answers by combining model reasoning with real-world data\n",
    "\n",
    "You are about to connect powerful components:\n",
    "\n",
    "- Azure OpenAI (for embeddings and LLM chat completions)\n",
    "- PostgreSQL with Vector and Graph extensions (for fast semantic and graph search)\n",
    "- APIs for real-world data (historical weather evidence)\n",
    "\n",
    "As you progress, each section of code will incrementally build up these capabilities, and by the final step, you’ll have a highly capable legal research assistant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058ee98",
   "metadata": {},
   "source": [
    "### Part 3.2: Setup the Agent App Python imports\n",
    "\n",
    "> **Note:** In your lab environment, we already have the PIP packages pre-deployed that are needed by the import statements in the following code block, so you do not need to install these.  But just for reference and for future usage of this code, here are the packages used:\n",
    "> - PostgreSQL connectivity (`psycopg`, `psycopg-binary`, `psycopg-pool`)\n",
    "> - Modeling and validation (`pydantic`)\n",
    "> - OpenAI and Semantic Kernel integration (`openai`, `semantic-kernel`)\n",
    "> - Notebook compatibility (`nest_asyncio`, `ipykernel`)\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "This set of imports prepares the technical foundation for building an AI-powered agent that interacts with a PostgreSQL database and OpenAI services. `nest_asyncio` is used to allow nested event loops, which is important when running asynchronous code inside a Jupyter notebook. Core Python modules like `os`, `asyncio`, `uuid`, and `requests` handle environment access, asynchronous execution, unique ID generation, and external API calls, respectively. `psycopg2` provides database connectivity to PostgreSQL, while `pydantic` offers structured data validation and modeling.\n",
    "\n",
    "The Semantic Kernel libraries enable creation of intelligent agents (`ChatCompletionAgent`), define plugins and function tools (`kernel_function`), manage prompt settings, and handle retrieval-augmented memory through `PostgresMemoryStore` and `SemanticTextMemory`. Finally, Azure OpenAI integration components (`AzureChatCompletion`, `AzureTextEmbedding`) allow the agent to generate responses and embeddings using cloud-based AI models.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Run the cell below using the \"▶\" icon next to the cell.\n",
    "\n",
    "1. This will run the code and show the output below.  Since these are just imports, there is nothing to show at the end other than a check mark showing success.\n",
    "\n",
    "    > **Note:** The first time this code block is ran, it may take around 20-30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import psycopg\n",
    "import uuid\n",
    "import requests\n",
    "from typing import Annotated\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.functions import kernel_function, KernelArguments\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding import AzureTextEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194337c",
   "metadata": {},
   "source": [
    "### Part 3.3: Setup environmental connection variables\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In order for our agent to interact with both Azure OpenAI services and the PostgreSQL database, we need to configure a few environment-specific connection variables. Rather than manually retrieving these values from the Azure Portal, we provide a shortcut: simply run the script located at `./Scripts/get_env.ps1` to automatically extract the correct credentials. After running the script, copy the output values into the fields below.\n",
    "\n",
    "This setup allows the agent to securely authenticate API requests to Azure OpenAI (using `AZURE_OPENAI_*` variables) and connect to the database where our case law data is stored (using the `DB_CONFIG` dictionary).\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Open the `get_env.ps1` file and update the `resourceGroupName` to match your lab environment. It should be like \"SKAgents-XXXXX\".\n",
    "1. Within VS Code, open a new terminal, and at the following path, enter:\n",
    "\n",
    "    `PS C:\\Users\\LabUser\\Downloads\\pg-sk-agents-lab> .\\Scripts\\get_env.ps1`\n",
    "\n",
    "1. From the output of the script in the terminal, copy the values for the following each into the variables in the code block below:\n",
    "    - `AZURE_OPENAI_ENDPOINT`\n",
    "    - `AZURE_OPENAI_KEY`\n",
    "    - `DB_CONFIG - HOST`\n",
    "    - `DB_CONFIG - PASSWORD`\n",
    "\n",
    "    > **Note:** For `DB_CONFIG - PASSWORD`, this is a very long string due to being an Entra ID Access Token, be sure to copy the entire string as the password.\n",
    "\n",
    "1. For the value for `{USER}`, this will be your Lab Username Credential, you can get these from the Lab Manual, in Part 3.3\n",
    "\n",
    "1. Once you have each of these fields filled in, then run the cell below using the \"▶\" icon next to the cell.  This will run the code and show the output below.  You will reuse these variables and objects throughout the lab notebook below.\n",
    "\n",
    "1. You should see a check mark when it completes, showing success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56021df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_ENDPOINT   = \"\"\n",
    "AZURE_OPENAI_KEY        = \"\"\n",
    "AZURE_OPENAI_DEPLOYMENT = \"gpt-4o\"\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\":     \"\",\n",
    "    \"dbname\":   \"cases\",\n",
    "    \"user\":     \"\",\n",
    "    \"password\": \"\",\n",
    "    \"port\": \"5432\",\n",
    "    \"sslmode\":  \"require\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a3fd1",
   "metadata": {},
   "source": [
    "### Part 3.4: Create Semantic Kernel Plugin for Basic Database Queries\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this step, we create a custom plugin called DatabaseSearchPlugin to give our agent the ability to interact directly with the case law database using basic SQL queries. This plugin uses the psycopg2 library to establish a connection to PostgreSQL, execute queries, and return results. We define two simple but important functions: `count_cases()` to return the total number of cases in the database, and `search_cases(keyword)` to perform a keyword search against case opinions. Each function is decorated with `@kernel_function`, which registers it as a callable tool within the Semantic Kernel framework. This makes these database operations available to the agent automatically during conversation, enabling the agent to retrieve real-time, grounded information from our dataset.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review the code below, notice the print statements outputting to the terminal the name of the function when it is called.  This will be helpful later when we run the agent, and we want to see what functions it chose to call.\n",
    "\n",
    "1. Run the cell below using the \"▶\" icon next to the cell.\n",
    "\n",
    "1. This will run the code and show the output below.  Since these are just imports, there is nothing to show at the end other than a check mark showing success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseSearchPlugin:\n",
    "        def __init__(self, cfg):\n",
    "            self.cfg = cfg\n",
    "\n",
    "        @kernel_function(description=\"Return the total number of cases in the database.\")\n",
    "        def count_cases(self) -> str:\n",
    "            \n",
    "            # print statement so we can see when this function chosen by the agent and is called\n",
    "            print(\"count_cases was called\")\n",
    "            \n",
    "            conn = psycopg.connect(**self.cfg)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"SELECT COUNT(*) FROM cases;\")\n",
    "            n = cur.fetchone()[0]\n",
    "            conn.close()\n",
    "            return str(n)\n",
    "\n",
    "        @kernel_function(description=\"Find up to 10 case IDs and names whose opinion contains the given keyword.\")\n",
    "        def search_cases(self, keyword: str) -> str:\n",
    "            \n",
    "            # print statement so we can see when this function chosen by the agent and is called\n",
    "            print(\"search_cases was called\")\n",
    "            \n",
    "            conn = psycopg.connect(**self.cfg)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\n",
    "                \"SELECT id, name, opinion FROM cases WHERE opinion ILIKE %s LIMIT 10;\",\n",
    "                (f\"%{keyword}%\",)\n",
    "            )\n",
    "            rows = cur.fetchall()\n",
    "            conn.close()\n",
    "            if not rows:\n",
    "                return \"No matches\"\n",
    "            return \"\\n\".join(f\"{r[0]}: {r[1]}: {r[2][:1000]}\" for r in rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0006bb",
   "metadata": {},
   "source": [
    "### Part 3.5: Test Run of our New Agent\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "Now that we have created our first plugin, we're ready to assemble and test an initial version of our agent. In this step, we create a default `OpenAIChatPromptExecutionSettings` to define basic settings. We then create an instance of `AzureChatCompletion`, which serves as the underlying communication layer between our agent and Azure OpenAI. Using these components, we instantiate a `ChatCompletionAgent`, providing it a name, a set of behavioral instructions, and a list of available plugins (in this case, just `DatabaseSearchPlugin`).\n",
    "\n",
    "Finally, we send a sample user query to the agent and retrieve its response. This test validates that our agent can successfully invoke plugin functions, query the database, and integrate the results into a natural language reply.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review the code below, notice the following:\n",
    "    - Inside `ChatCompletionAgent` we define `instructions` which act as notion of a \"system prompt\" for the Agent to define its purpose and goals\n",
    "    - For now we are passing in our `DatabaseSearchPlugin` class, we will be creating more PlugIns to enhance the functionality of our agent in the next steps\n",
    "    - Notice the `user_query` variable, and how it is asking how many cases there are, plus about water leaking cases.\n",
    "\n",
    "1. Run the cell below using the \"▶\" icon next to the cell.  This invokes the agent and subsequent LLM calls, this may take a few moments to run.\n",
    "\n",
    "1. After the code runs, notice the following:\n",
    "    - There should have been 2 functions called: `count_cases` and `search_cases`\n",
    "    - This happened because the agent interpreted the prompt and made a decision to call these 2 functions\n",
    "    - Notice how we got back a clear response of 377 cases are in our database.  This was based on our `count_cases` database function giving the LLM grounded truth about our dataset.\n",
    "    - Lastly, notice how we asked for 10 cases, but only got 2.  This is because our `search_cases` function is just the ILIKE operator and not yet using a vector search.  It could only find 2 cases that matching using the basic keyword ILIKE search.  In our next lab sections, we will see how we can improve on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = OpenAIChatPromptExecutionSettings()\n",
    "    \n",
    "chat_svc = AzureChatCompletion(\n",
    "    deployment_name=AZURE_OPENAI_DEPLOYMENT,\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_KEY)\n",
    "\n",
    "# Create agent with plugin and settings\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_svc,\n",
    "    name=\"SK-Assistant\",\n",
    "    instructions=\"You are a helpful legal assistant.  Respond to the user with the name of the cases and reasoning why the cases are the most relevant, and a short sentence summary of the opinion of the cases.\",\n",
    "    plugins=[DatabaseSearchPlugin(DB_CONFIG)],\n",
    "    arguments=KernelArguments(settings)\n",
    ")\n",
    "\n",
    "user_query = \"How many cases are there, and find me 10 cases regarding the notion of water leaking.\"\n",
    "\n",
    "print(\"// Functions the Agent Called: //\")\n",
    "\n",
    "response = await agent.get_response(messages=user_query)\n",
    "\n",
    "print(\"// Agent Response: //\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1206c174",
   "metadata": {},
   "source": [
    "### Part 3.6: Improve Agent Accuracy by Adding Semantic Re-ranking Query Plugin\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this step, we add a new plugin called `SemanticRerankingPlugin` to increase the precision of our agent’s search results. Instead of relying only on keyword matching, this plugin uses semantic search and re-ranking to evaluate results based on the meaning and intent of the user query.\n",
    "\n",
    "It works in two phases: first, it performs a vector similarity search using Azure OpenAI embeddings to find approximately relevant cases; then, it reorders these using a separate model (e.g., `semantic_relevance`) that scores and ranks the results for deeper semantic alignment. This two-step approach helps the agent prioritize results that are not just textually similar, but also topically and contextually relevant—making it ideal for legal queries where nuance matters. As with other tools, this function is registered using `@kernel_function`, so the agent can intelligently decide when to use it.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Before running this code block, we need to run a SQL script on the database.\n",
    "    - In VS Code, in the folder structure, in the folder `./Scripts/` navigate to the `setup_reranker.sql` file\n",
    "    - Once inside the file, still in VS Code, press on the keyboard `CTRL+SHIFT+P` to open the VS Code action panel, type `PGSQL Connect` and select the top `PGSQL: Connect` option\n",
    "    - Once prompted, select the Connection you made in the earlier steps in the lab\n",
    "    - You will then be asked the port number for the Connection, just hit `enter` to accept the default\n",
    "    - You should now be Connected to your database in the `setup_reranker.sql` file\n",
    "    - Next we need to replace the `{ENDPOINT}` and `{KEY}` tokens with OPenAI  Key and Endpoint\n",
    "    - In the editor window, click the \">\" button in the top right to run the script\n",
    "    - This will setup our connection to Azure ML via our `azure_ai` PostgreSQL database extension, and create 2 PostgresSQL PL/SQL functions needed for semantic re-ranking\n",
    "\n",
    "#### Review the code:\n",
    "\n",
    "🔍 Plugin Explanation — search_semantic_reranked_cases\n",
    "\n",
    "This plugin introduces the function search_semantic_reranked_cases, designed to deliver highly relevant legal case results by understanding the semantic meaning of the query, rather than relying solely on basic keyword or vector similarity.\n",
    "\n",
    "   - ✅ How the Plugin Works\n",
    "The function is decorated with @kernel_function, which registers it as a callable semantic skill within Semantic Kernel. This means the agent can automatically invoke it based on the user's intent — especially when prompts include phrases like \"high accuracy is important\" or when dealing with complex legal search needs.\n",
    "\n",
    "   - 🔁 Two-Phase Search Strategy\n",
    "This hybrid approach combines vector search and ML-based reranking to ensure quality and context-awareness:\n",
    "\n",
    "      Phase 1: Vector Similarity Search (PostgreSQL)\n",
    "The plugin first queries the cases table using azure_openai.create_embeddings() to embed the user’s input, then retrieves the top 60 most similar cases using Postgres’s <=> vector distance.\n",
    "\n",
    "      Phase 2: Semantic Re-Ranking (Local ML with CrossEncoder)\n",
    "The retrieved candidates are then passed into a locally running sentence-transformers CrossEncoder model. It evaluates each candidate’s textual content in context with the query, and scores them based on semantic relevance. The top 10 results are returned, sorted by meaning, not just distance.\n",
    "\n",
    "   - 🧠 Why This Matters\n",
    "This plugin improves retrieval performance in cases where:\n",
    "\n",
    "      - Keywords may differ but intent is the same (e.g., “leaking ceiling” vs “water from upstairs apartment”)\n",
    "\n",
    "      - Legal phrases or abstract issues require contextual understanding\n",
    "\n",
    "      - You need precise and ranked results, such as for court cases, contract disputes, or risk assessment\n",
    "\n",
    "   - 🆚 Compared to Basic Search\n",
    "     The earlier search_cases() plugin (likely based on SQL ILIKE or vector-only ranking) may work for direct or fuzzy keyword matches, but it struggles with:\n",
    "\n",
    "     - Synonyms or paraphrased queries\n",
    "\n",
    "     - Incomplete or ambiguous input\n",
    "\n",
    "     - Domain-specific nuances (like legal intent or precedence)\n",
    "\n",
    "       This plugin enhances both precision and recall, and will automatically be selected by the agent when the user expresses needs like \"accurate\", \"relevant\", \"meaningful\", or \"best matching\" cases.\n",
    "\n",
    "Finally, run the code block cell by clicking the \"▶\" button on the left side of the code block.\n",
    "\n",
    " - This will run the code but there will be no output yet.\n",
    " - Since this is just a class definition, there is nothing to show at the end other than a check mark showing success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the required library is installed\n",
    "!pip install -q sentence-transformers\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "import psycopg\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "class SemanticRerankingPlugin:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        # Load CrossEncoder model (you can choose a more accurate one if needed)\n",
    "        self.model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    @kernel_function(description=\"Use semantic re-ranking function to query and find cases matching the query based on semantic intent and relevance. Use this function when high accuracy is needed.\")\n",
    "    def search_semantic_reranked_cases(self, query: str) -> str:\n",
    "        print(\"search_semantic_reranked_cases was called\")\n",
    "\n",
    "        conn = psycopg.connect(**self.cfg)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Step 1–2: Vector Search to fetch top 60 candidate cases\n",
    "        cur.execute(\"\"\"\n",
    "            WITH embedding_query AS (\n",
    "                SELECT azure_openai.create_embeddings('text-embedding-3-small', %s)::vector AS embedding\n",
    "            )\n",
    "            SELECT id, name, opinion\n",
    "            FROM cases, embedding_query\n",
    "            ORDER BY opinions_vector <=> embedding\n",
    "            LIMIT 60;\n",
    "        \"\"\", (query,))\n",
    "        rows = cur.fetchall()\n",
    "        conn.close()\n",
    "\n",
    "        if not rows:\n",
    "            return \"No matches found.\"\n",
    "\n",
    "        # Step 3: Rerank using CrossEncoder\n",
    "        case_texts = [f\"{r[1]}: {r[2]}\" for r in rows]\n",
    "        pairs = [(query, text) for text in case_texts]\n",
    "        scores = self.model.predict(pairs)\n",
    "\n",
    "        # Step 4: Sort by score\n",
    "        reranked = sorted(zip(rows, scores), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        # Step 5: Format results safely\n",
    "        return \"\\n\".join(\n",
    "            f\"{row[0]}: {row[1]} (score: {round(score, 3)}): {row[2][:500]}...\"\n",
    "            for row, score in reranked\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1491f",
   "metadata": {},
   "source": [
    "### Part 3.7: Add a GraphRAG Query PlugIn to the Agent for Additional Accuracy Improvements\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this step, we build another advanced plugin, `GraphDatabasePlugin`, that combines vector search with graph analysis to find the most influential cases related to a query topic. The `search_graph_cases` method first uses an embedding-based similarity search to semantically rank cases from the cases table. This ensures that only cases meaningfully related to the query are considered further.\n",
    "\n",
    "After narrowing the results semantically, the query leverages **Apache AGE** - a PostgreSQL extension that enables property graph querying via Cypher syntax. Specifically, it matches citations (relationships between cases) in the `case_graph` graph. By counting the number of incoming edges (citations) for each case, we can rank cases based on their influence or importance within the graph. Cases with more citations are prioritized, resulting in a more nuanced retrieval that considers both semantic relevance and citation authority.\n",
    "\n",
    "This hybrid retrieval technique is an example of **GraphRAG (Graph Retrieval-Augmented Generation)** and represents a more sophisticated form of grounded information retrieval for legal, academic, or research applications.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Before running this code block, we need to run a SQL script on the database.\n",
    "    - In VS Code, in the folder structure, in the folder `./Scripts/` navigate to the `create_graph.sql` file\n",
    "    - Once inside the file, still in VS Code, press on the keyboard `CTRL+SHIFT+P` to open the VS Code action panel, type `PGSQL Connect` and select the top `PGSQL: Connect` option\n",
    "    - Once prompted, select the Connection you made in the earlier steps in the lab\n",
    "    - You will then be asked the port number for the Connection, just hit `enter` to accept the default\n",
    "    - You should now be Connected to your database in the `create_graph.sql` file\n",
    "    - In the editor window, click the \">\" button in the top right to run the script\n",
    "    - This will build our graph database via the Apache AGE extension in our Azure PostgreSQL database using our loaded 377 legal cases\n",
    "\n",
    "1. Next, because we are using the Apache AGE PostgreSQL extension to provide us Graph database capabilities, we need to enable the extension on our database.\n",
    "    - Run the following PowerShell script within VS Code\n",
    "    - Within VS Code, open a new terminal, and at the following path, enter:\n",
    "\n",
    "        `PS C:\\Users\\LabUser\\Downloads\\pg-sk-agents-lab> .\\Scripts\\load_age.ps1`\n",
    "\n",
    "        > **Note:** This will run through 3 main commands, all together will take around 60-120 seconds.\n",
    "\n",
    "1. Review the code:\n",
    "    - This plugin allows the agent to find legal cases that are not only semantically similar to a user's query but also highly cited by other cases - providing both relevance and legal importance. The `@kernel_function` decorator makes this method callable by the agent.\n",
    "    - Look at the `semantic_ranked` CTE (Common Table Expression). This ranks cases by their similarity to the input query using embedding-based vector comparison (<=>). The function limits results to the top 60 most semantically similar opinions using Azure OpenAI’s embedding model.\n",
    "    - Examine the graph CTE. It runs a Cypher query on the `case_graph` to count how many times each case is cited by others. These citation counts are joined to the semantic results using the case ID. This allows the plugin to prioritize not just relevant cases, but those that are also influential in the citation network.\n",
    "    - The final `SELECT` returns 10 cases with the highest number of citations among the semantically relevant ones. The list is ordered by refs `DESC`, meaning more citations come first. Each opinion is truncated to the first 1000 characters to keep responses concise.\n",
    "\n",
    "1. Finally, run the code block cell by clicking the \"▶\" button on the left side of the code block.\n",
    "    - This will run the code but there will be no output yet.\n",
    "    - Since this is again just a class definition, there is nothing to show at the end other than a check mark showing success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ec57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDatabasePlugin:\n",
    "        def __init__(self, cfg):\n",
    "            # Store the database configuration dictionary\n",
    "            self.cfg = cfg\n",
    "\n",
    "        @kernel_function(description=\"Use an advanced accuracy query to find important cases with high levels of citations about the query topic.\")\n",
    "        def search_graph_cases(self, query: str) -> str:\n",
    "            \n",
    "            # Debug log to indicate function was triggered\n",
    "            print(\"search_graph_cases was called\")\n",
    "            \n",
    "            # Connect to the PostgreSQL database using the provided config\n",
    "            conn = psycopg.connect(**self.cfg)\n",
    "            cur = conn.cursor()\n",
    "\n",
    "            # Set the search path first\n",
    "            # Step 1: Set the schema search path to include Apache AGE graph catalog\n",
    "            cur.execute('SET search_path = public, ag_catalog, \"$user\";')\n",
    "\n",
    "            # Execute a compound query using semantic search + graph analysis (Apache AGE)\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                -- // Step 2: Rank cases semantically using embedding similarity //\n",
    "                WITH semantic_ranked AS (\n",
    "                    SELECT id, name, opinion, opinions_vector\n",
    "                    FROM cases\n",
    "                    ORDER BY opinions_vector <=> azure_openai.create_embeddings('text-embedding-3-small', %s)::vector\n",
    "                    LIMIT 60\n",
    "                ),\n",
    "                -- // Step 3: Use a Cypher graph query to count how many citations (edges) each case has //\n",
    "                graph AS (\n",
    "                    SELECT graph_query.refs, semantic_ranked.*, graph_query.case_id \n",
    "                    FROM semantic_ranked\n",
    "                    LEFT JOIN cypher('case_graph', $$\n",
    "                        MATCH ()-[r]->(n)\n",
    "                        RETURN n.case_id, COUNT(r) AS refs\n",
    "                    $$) as graph_query(case_id TEXT, refs BIGINT)\n",
    "                    ON semantic_ranked.id = graph_query.case_id::int\n",
    "                )\n",
    "                -- // Step 4: Return the top 10 cases, prioritized by number of citations (refs) //\n",
    "                SELECT id, name, opinion\n",
    "                FROM graph\n",
    "                ORDER BY refs DESC NULLS LAST\n",
    "                LIMIT 10;\n",
    "                \"\"\", \n",
    "                (query,)\n",
    "            )\n",
    "            rows = cur.fetchall() # Fetch all resulting rows\n",
    "            conn.close() # Close the database connection\n",
    "\n",
    "            # Return either \"No matches\" or a formatted list of results\n",
    "            if not rows:\n",
    "                return \"No matches\"\n",
    "            return \"\\n\".join(f\"{r[0]}: {r[1]}: {r[2][:1000]}\" for r in rows) # Truncate long opinions to 1000 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3164fd",
   "metadata": {},
   "source": [
    "### Part 3.8: Re-Assemble our Agent with new advanced PlugIns and Re-Test\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this step, we re-assemble the full agent by attaching all of the custom plugins we’ve created so far: `DatabaseSearchPlugin`, `SemanticRerankingPlugin`, and `GraphDatabasePlugin`. These plugins give the agent access to different querying strategies, from simple keyword searches to advanced semantic filtering and graph-based citation analysis. By registering all plugins in the plugins list, we enable the agent to choose the right tool based on the intent expressed in the user’s prompt.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review the code below, notice the following:\n",
    "    - We only need to re-define our agent object using the `ChatCompletionAgent` class\n",
    "    - Notice we are now adding our new PlugIns on the line:\n",
    "        - `plugins=[DatabaseSearchPlugin(DB_CONFIG),SemanticRerankingPlugin(DB_CONFIG),GraphDatabasePlugin(DB_CONFIG)],`    \n",
    "    - Notice the `user_query` variables, there are some additional ones you can try testing yourself by uncommenting one at a time, then re-running the code cell\n",
    "\n",
    "1. Run the cell below using the \">\" icon next to the cell.  This invokes the agent and subsequent LLM calls, this may take a few moments to run.\n",
    "\n",
    "1. After the code runs, notice the following:\n",
    "    - Depending on the prompt you chosen, there should have been between 2-3 functions called, such as: `search_graph_cases` and `search_semantic_reranked_cases`\n",
    "    - Notice how we asked for 10 cases, and now got 10 cases. This is because the we are now using semantic vector search, not just keyword search for the database queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Create agent with new plugins\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_svc,\n",
    "    name=\"SK-Assistant\",\n",
    "    instructions=\"You are a helpful legal assistant.  Respond to the user with the name of the cases and reasoning why the cases are the most relevant, and a short sentence summary of the opinion of the cases.\",\n",
    "    plugins=[DatabaseSearchPlugin(DB_CONFIG),SemanticRerankingPlugin(DB_CONFIG),GraphDatabasePlugin(DB_CONFIG)],\n",
    "    arguments=KernelArguments(settings)\n",
    ")\n",
    "\n",
    "# Try these different prompts to see how the agent responds:\n",
    "# Remove one comment at a time to test different prompts\n",
    "user_query = \"Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.  Also how many cases are there overall?\"\n",
    "#user_query = \"Bring into 1 list of 10 cases, ranked by relevancy -- Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.\"\n",
    "#user_query = \"How many cases are there, and high accuracy is important, help me find 10 highly relevant cases related to water leaking in my apartment.\"\n",
    "\n",
    "print(\"// Functions the Agent Called: //\")\n",
    "\n",
    "response = await agent.get_response(messages=user_query)\n",
    "\n",
    "print(\"// Agent Response: //\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32517a",
   "metadata": {},
   "source": [
    "### Part 3.9: Adding a Weather PlugIn to the Agent\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "To enhance our legal assistant with real-world context, we introduce a `WeatherPlugin` that enables the agent to retrieve historical weather data (specifically rainfall) based on a given date and geographic location. This is especially useful in real estate or tenant-landlord disputes where weather-related damage (e.g., leaks or flooding) may be a legal factor. The plugin uses the Open-Meteo Archive API, a free and reliable weather data service.\n",
    "\n",
    "When a user prompt mentions a need for weather data—such as \"What was the rainfall on Feb 1, 2024, in Seattle?\"—the agent will automatically call this function. The returned data provides accurate, grounded evidence that enhances the agent’s response and credibility.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review the code below and observe the following:\n",
    "    - The `@kernel_function` decorator registers this function so the agent can call it based on user intent.\n",
    "    - The plugin uses `requests.get()` to make a live API call to Open-Meteo, passing latitude, longitude, and date parameters.\n",
    "    - The response is parsed from JSON and extracts the precipitation value from the `daily.precipitation_sum` array.\n",
    "    - The result is returned as a readable string with the date, coordinates, and rainfall in millimeters.\n",
    "\n",
    "1. Run the cell below using the \"▶\" (Run) button. There is no visible output until the function is called by the agent in a relevant prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9190f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherPlugin:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Get total precipitation (in mm) on a given date and location (latitude, longitude).\"\n",
    "    )\n",
    "    def get_historical_rainfall(self, date: str, latitude: float, longitude: float) -> str:\n",
    "        \"\"\"\n",
    "        date: YYYY-MM-DD\n",
    "        latitude, longitude: WGS84 coords\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"get_historical_rainfall was called\")\n",
    "\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"start_date\": date,\n",
    "            \"end_date\":   date,\n",
    "            \"daily\":      \"precipitation_sum\",\n",
    "            \"timezone\":   \"UTC\"\n",
    "        }\n",
    "        resp = requests.get(url, params=params)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        data = resp.json()\n",
    "        # the API returns a list for daily.precipitation_sum\n",
    "        rain_mm = data[\"daily\"][\"precipitation_sum\"][0]\n",
    "        return f\"On {date} at ({latitude}, {longitude}), total precipitation was {rain_mm} mm.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613a5df",
   "metadata": {},
   "source": [
    "### Part 3.10: Add our new Weather PlugIn to our Agent and Re-Test\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this step, we complete our agent by including the new `WeatherPlugin` alongside our database and semantic plugins. This enables the agent to answer more complex, multi-part prompts that require both legal case analysis and external factual grounding—such as historical rainfall on a specific date and location.\n",
    "\n",
    "When a user prompt mentions weather-related conditions (e.g., \"*What was the rainfall on February 1, 2024, in Seattle?*\"), the agent can automatically call the appropriate plugin function. Semantic Kernel handles function selection based on the natural language intent, so all tools are available simultaneously without manual switching. This demonstrates the power of multi-plugin orchestration in agent design—allowing a single agent to reason across legal data and real-world APIs in one coherent response.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review how the `WeatherPlugin()` is added to the list of available plugins in the `ChatCompletionAgent`.\n",
    "\n",
    "1. Examine the user_query. Note how it includes both:\n",
    "    - A factual request (weather evidence),\n",
    "    - And a legal research task (finding relevant legal cases).\n",
    "\n",
    "1. Run the cell below using the \"▶\" (Run) button and observe:\n",
    "    - Which functions the agent chooses to call (check your logs or printed output),\n",
    "    - How the agent combines different results into a single response.\n",
    "    - Try editing the user_query to include other dates or cities and observe how the weather data changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b672c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Create agent with new plugins\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_svc,\n",
    "    name=\"SK-Assistant\",\n",
    "    instructions=\"You are a helpful legal assistant.  Respond to the user with the name of the cases and reasoning why the cases are the most relevant, and a short sentence summary of the opinion of the cases.\",\n",
    "    plugins=[DatabaseSearchPlugin(DB_CONFIG),SemanticRerankingPlugin(DB_CONFIG),GraphDatabasePlugin(DB_CONFIG),WeatherPlugin()],\n",
    "    arguments=KernelArguments(settings)\n",
    ")\n",
    "\n",
    "# Try these different prompts to see how the agent responds:\n",
    "# Remove one comment at a time to test different prompts\n",
    "user_query = \"What was the rainfall on 2024-02-01 in Seattle, WA? I need this for evidence.  Also High accuracy is important, help me find 10 highly relevant cases related to water leaking in my clients apartment.\"\n",
    "#user_query = \"I am a real estate lawyer, so cases need to be related to real estate law.\"\n",
    "#user_query = \"Find me 10 cases regarding the notion of water leaking.\"\n",
    "#user_query = \"How many cases are there, and high accuracy is important, help me find 10 highly relevant cases related to water leaking in my apartment.\"\n",
    "#user_query = \"Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.  Also how many cases are there overall?\"\n",
    "#user_query = \"Bring into 1 list of 10 cases, ranked by relevancy -- Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.\"\n",
    "#user_query = \"How many cases are in my database? What was the rainfall on 2024-02-01 in Seattle, WA? I need this for evidence.  Also High accuracy is important, help me find 10 highly relevant cases related to water leaking in my clients apartment.\"\n",
    "\n",
    "print(\"// Functions the Agent Called: //\")\n",
    "response = await agent.get_response(messages=user_query)\n",
    "\n",
    "print(\"// Agent Response: //\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784b47b",
   "metadata": {},
   "source": [
    "### Part 3.11: Adding Memory into the Agent\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this final step, we complete our agent’s capabilities by enabling semantic memory using SemanticTextMemory backed by a PostgreSQL vector store. This allows the agent to store and recall relevant prior user queries and its own responses—creating a more personalized and context-aware experience over time.\n",
    "\n",
    "When a new prompt is received, the agent retrieves semantically similar past queries from memory using embedding-based vector search. These memories are then prepended to the current prompt, giving the agent important context and continuity across interactions. This memory is especially useful in real-world legal scenarios where a user may build a case over several prompts, and the agent must retain prior details to offer more precise, relevant guidance.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review how Custom PostgresMemoryStore Class and AzureTextEmbedding are used to set up vector-based memory for the agent.\n",
    "\n",
    "1. Examine how the user query is stored into memory, and how top related queries are retrieved with semantic search.\n",
    "\n",
    "1. Observe how the memory context is prepended to the new prompt before being passed to the agent.\n",
    "\n",
    "1. Run the cell below using the \"▶\" (Run) button and inspect:\n",
    "    - How the agent’s output incorporates the memory context,\n",
    "    - What gets saved to memory as both the query and the agent’s reply,\n",
    "    - How this memory layer enables continuity in multi-turn conversations.\n",
    "\n",
    "1. Test running through the first 3 sample prompts below, by uncommenting a single prompt, one at a time\n",
    "    - As you run through these, this helps to showcase the memory functionality as the Agent keeps track of these facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba93fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from psycopg_pool import ConnectionPool\n",
    "from psycopg.sql import SQL, Identifier\n",
    "from semantic_kernel.memory.memory_store_base import MemoryStoreBase\n",
    "from semantic_kernel.memory.memory_record import MemoryRecord\n",
    "from semantic_kernel.exceptions.memory_connector_exceptions import MemoryConnectorInitializationError\n",
    "from semantic_kernel.connectors.postgres import PostgresSettings, DEFAULT_SCHEMA\n",
    "\n",
    "\n",
    "class PostgresMemoryStore(MemoryStoreBase):\n",
    "    def __init__(self, connection_string: str, default_dimensionality: int, schema: str = DEFAULT_SCHEMA):\n",
    "        try:\n",
    "            settings = PostgresSettings(connection_string=connection_string)\n",
    "        except Exception as ex:\n",
    "            raise MemoryConnectorInitializationError(\"Failed to create Postgres settings.\", ex) from ex\n",
    "\n",
    "        self._connection_pool = ConnectionPool(\n",
    "            open=True, kwargs=settings.get_connection_args()\n",
    "        )\n",
    "        self._schema = schema\n",
    "        self._default_dimensionality = default_dimensionality\n",
    "\n",
    "    async def create_collection(self, collection_name: str, dimension_num: int = None):\n",
    "        dim = dimension_num or self._default_dimensionality\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {scm}.{tbl} (\n",
    "                    key TEXT PRIMARY KEY,\n",
    "                    embedding vector({dim}),\n",
    "                    metadata JSONB,\n",
    "                    timestamp TIMESTAMP\n",
    "                )\n",
    "            \"\"\").format(scm=Identifier(self._schema), tbl=Identifier(collection_name), dim=dim))\n",
    "\n",
    "    async def does_collection_exist(self, collection_name: str) -> bool:\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT EXISTS (\n",
    "                    SELECT FROM information_schema.tables \n",
    "                    WHERE table_schema = %s AND table_name = %s\n",
    "                )\n",
    "            \"\"\", (self._schema, collection_name))\n",
    "            return cur.fetchone()[0]\n",
    "\n",
    "    async def get_collections(self) -> list[str]:\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT table_name FROM information_schema.tables\n",
    "                WHERE table_schema = %s\n",
    "            \"\"\", (self._schema,))\n",
    "            return [row[0] for row in cur.fetchall()]\n",
    "\n",
    "    async def get(self, collection_name: str, key: str) -> MemoryRecord | None:\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(\"\"\"\n",
    "                SELECT key, embedding, metadata, timestamp FROM {scm}.{tbl}\n",
    "                WHERE key = %s\n",
    "            \"\"\").format(scm=Identifier(self._schema), tbl=Identifier(collection_name)), (key,))\n",
    "            row = cur.fetchone()\n",
    "            if not row:\n",
    "                return None\n",
    "            meta = row[2]\n",
    "            return MemoryRecord.local_record(\n",
    "                id=row[0],\n",
    "                embedding=np.array(row[1]),\n",
    "                text=meta[\"text\"],\n",
    "                description=meta[\"description\"],\n",
    "                additional_metadata=meta.get(\"additional_metadata\"),\n",
    "                timestamp=row[3]\n",
    "            )\n",
    "\n",
    "    async def get_batch(self, collection_name: str, keys: list[str]) -> list[MemoryRecord]:\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(f\"\"\"\n",
    "                SELECT key, embedding, metadata, timestamp FROM {self._schema}.{collection_name}\n",
    "                WHERE key = ANY(%s)\n",
    "            \"\"\"), (keys,))\n",
    "            rows = cur.fetchall()\n",
    "            return [\n",
    "                MemoryRecord.local_record(\n",
    "                    id=row[0],\n",
    "                    embedding=np.array(row[1]),\n",
    "                    text=row[2][\"text\"],\n",
    "                    description=row[2][\"description\"],\n",
    "                    additional_metadata=row[2].get(\"additional_metadata\"),\n",
    "                    timestamp=row[3]\n",
    "                )\n",
    "                for row in rows\n",
    "            ]\n",
    "\n",
    "    async def upsert(self, collection_name: str, record: MemoryRecord) -> str:\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(\"\"\"\n",
    "                INSERT INTO {scm}.{tbl} (key, embedding, metadata, timestamp)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (key) DO UPDATE\n",
    "                SET embedding = EXCLUDED.embedding,\n",
    "                    metadata = EXCLUDED.metadata,\n",
    "                    timestamp = EXCLUDED.timestamp\n",
    "                RETURNING key\n",
    "            \"\"\").format(scm=Identifier(self._schema), tbl=Identifier(collection_name)),\n",
    "            (\n",
    "                record._id,\n",
    "                record.embedding.tolist(),\n",
    "                json.dumps({\n",
    "                    \"text\": record._text,\n",
    "                    \"description\": record._description,\n",
    "                    \"additional_metadata\": record._additional_metadata\n",
    "                }),\n",
    "                record._timestamp,\n",
    "            ))\n",
    "            return cur.fetchone()[0]\n",
    "\n",
    "    async def upsert_batch(self, collection_name: str, records: list[MemoryRecord]) -> list[str]:\n",
    "        return [await self.upsert(collection_name, r) for r in records]\n",
    "\n",
    "    async def remove(self, collection_name: str, key: str):\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(\"\"\"\n",
    "                DELETE FROM {scm}.{tbl}\n",
    "                WHERE key = %s\n",
    "            \"\"\").format(scm=Identifier(self._schema), tbl=Identifier(collection_name)), (key,))\n",
    "\n",
    "    async def remove_batch(self, collection_name: str, keys: list[str]):\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(f\"\"\"\n",
    "                DELETE FROM {self._schema}.{collection_name}\n",
    "                WHERE key = ANY(%s)\n",
    "            \"\"\"), (keys,))\n",
    "\n",
    "    async def delete_collection(self, collection_name: str):\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(\"DROP TABLE IF EXISTS {scm}.{tbl}\")\n",
    "                        .format(scm=Identifier(self._schema), tbl=Identifier(collection_name)))\n",
    "\n",
    "    async def get_nearest_matches(self, collection_name: str, embedding: np.ndarray, limit: int, min_relevance_score: float = 0.0, with_embeddings: bool = False):\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(f\"\"\"\n",
    "                SELECT key, embedding, metadata, 1 - (embedding <=> %s::vector) AS score, timestamp\n",
    "                FROM {self._schema}.{collection_name}\n",
    "                ORDER BY score DESC\n",
    "                LIMIT {limit}\n",
    "            \"\"\"), (embedding.tolist(),))\n",
    "            rows = cur.fetchall()\n",
    "            return [\n",
    "                (MemoryRecord.local_record(\n",
    "                    id=row[0],\n",
    "                    embedding=np.array(row[1]) if with_embeddings else np.array([]),\n",
    "                    text=row[2][\"text\"],\n",
    "                    description=row[2][\"description\"],\n",
    "                    additional_metadata=row[2].get(\"additional_metadata\"),\n",
    "                    timestamp=row[4],\n",
    "                ), row[3]) for row in rows\n",
    "            ]\n",
    "\n",
    "    async def get_nearest_match(self, collection_name: str, embedding: np.ndarray, min_relevance_score: float = 0.0, with_embedding: bool = False):\n",
    "        matches = await self.get_nearest_matches(collection_name, embedding, limit=1, min_relevance_score=min_relevance_score, with_embeddings=with_embedding)\n",
    "        return matches[0] if matches else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f914df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import uuid\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding import AzureTextEmbedding\n",
    "from semantic_kernel.memory.memory_store_base import MemoryStoreBase\n",
    "\n",
    "# 👇 Register your custom memory store if not already registered\n",
    "if not issubclass(PostgresMemoryStore, MemoryStoreBase):\n",
    "    MemoryStoreBase.register(PostgresMemoryStore)\n",
    "\n",
    "# === ✅ STEP 1: Define the user query ===\n",
    "user_query = (\n",
    "    \"Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above. \"\n",
    "    \"High accuracy is important, and high number of citations is important. Also how many cases are there overall?\"\n",
    ")\n",
    "\n",
    "# === ✅ STEP 2: PostgreSQL connection string ===\n",
    "conn_str = (\n",
    "    f\"host={DB_CONFIG['host']} \"\n",
    "    f\"port={DB_CONFIG['port']} \"\n",
    "    f\"dbname={DB_CONFIG['dbname']} \"\n",
    "    f\"user={DB_CONFIG['user']} \"\n",
    "    f\"password={DB_CONFIG['password']} \"\n",
    "    f\"sslmode={DB_CONFIG['sslmode']}\"\n",
    ")\n",
    "\n",
    "# === ✅ STEP 3: Initialize Postgres-backed memory store ===\n",
    "memory_store = PostgresMemoryStore(\n",
    "    connection_string=conn_str,\n",
    "    default_dimensionality=1536\n",
    ")\n",
    "\n",
    "# === ✅ STEP 3.5: Create collection if needed (first time setup)\n",
    "await memory_store.create_collection(\"agent_memories\")\n",
    "\n",
    "# === ✅ STEP 4: Azure embedding model for memory ===\n",
    "embedding_generator = AzureTextEmbedding(\n",
    "    deployment_name=\"text-embedding-3-small\",\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_KEY\n",
    ")\n",
    "\n",
    "# === ✅ STEP 5: Create the memory interface ===\n",
    "semantic_memory = SemanticTextMemory(\n",
    "    storage=memory_store,\n",
    "    embeddings_generator=embedding_generator\n",
    ")\n",
    "\n",
    "# === ✅ STEP 6: Save the new query to memory ===\n",
    "await semantic_memory.save_information(\n",
    "    collection=\"agent_memories\",\n",
    "    text=user_query,\n",
    "    id=str(uuid.uuid4()),\n",
    "    description=\"User query\"\n",
    ")\n",
    "\n",
    "# === ✅ STEP 7: Search for similar past queries ===\n",
    "recalls = await semantic_memory.search(\n",
    "    collection=\"agent_memories\",\n",
    "    query=user_query,\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "# === ✅ STEP 8: Format the recalled memory context ===\n",
    "memory_context = \"\\n\".join(f\"- {m.text}\" for m in recalls)\n",
    "\n",
    "# === ✅ STEP 9: Build the agent prompt ===\n",
    "prompt = (\n",
    "    f\"Here are things we’ve discussed before:\\n{memory_context}\\n\\n\"\n",
    "    f\"{user_query}\"\n",
    ")\n",
    "\n",
    "# === ✅ STEP 10: Query the agent with memory-enhanced prompt ===\n",
    "print(\"// Functions the Agent Called: //\")\n",
    "response = await agent.get_response(messages=prompt)\n",
    "\n",
    "# === ✅ STEP 11: Print prompt and response ===\n",
    "print(\"// Prompt with Added Memory Context: //\")\n",
    "print(prompt)\n",
    "print(\"// Agent Response: //\")\n",
    "print(response.content)\n",
    "\n",
    "# === ✅ STEP 12: Save the agent's reply back to memory ===\n",
    "await semantic_memory.save_information(\n",
    "    collection=\"agent_memories\",\n",
    "    text=str(response.content),\n",
    "    id=str(uuid.uuid4()),\n",
    "    description=\"Agent reply\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d1a23",
   "metadata": {},
   "source": [
    "## Congratulations you have completed the Lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9dba0",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We have curated additional resources to enhance your ongoing journey in building AI agents and AI-powered applications with Azure Database for PostgreSQL.\n",
    "\n",
    "- A more detailed blog post about the legal case example of lab in the [GraphRAG Solution for Azure Database for PostgreSQL](https://aka.ms/pg-graphrag) check the code in the [GitHub repository](https://aka.ms/postgres-graphrag-solution).\n",
    "- Learn more about [Graph data in Azure Database for PostgreSQL](https://aka.ms/age-blog).\n",
    "- Get familiar with the new [PostgreSQL extension for Visual Studio Code]().\n",
    "- Learn more about Semantic Ranking with the [Semantic Ranker Solution Accelerator](https://aka.ms/semantic-ranker-solution-accelerator-pg-blog) and its associated [GitHub repository](https://aka.ms/pg-ranker-repo).\n",
    "- Finally, our more recent solutions accelerator [Build your own advanced AI Copilot with Postgres](http://aka.ms/pg-byoac-docs) teaches you how to extract data from statements of work (SOWs) and invoices in PDF files and use AI to validate them, more details in the [GitHub repo](http://aka.ms/pg-byoac-repo)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
