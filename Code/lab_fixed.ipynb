{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d97f8e6",
   "metadata": {},
   "source": [
    "## Build an Agentic App with PostgreSQL, GraphRAG, and Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e253c9",
   "metadata": {},
   "source": [
    "### Part 3.1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7fafd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "In this notebook, you will build a Semantic Kernel Agent that can reason over our database of legal cases you deployed in the previous steps. You will also incorporate external web service data, and use memory to improve its responses over time.\n",
    "\n",
    "Semantic Kernel is an open-source SDK developed by Microsoft that helps developers create advanced AI agents by combining:\n",
    "\n",
    "- LLMs (Large Language Models) like OpenAI's GPT models\n",
    "- Plugins (custom tools and functions the agent can call)\n",
    "- Memory (saving and recalling past conversations or facts)\n",
    "\n",
    "An Agent in Semantic Kernel is a smart assistant that can:\n",
    "\n",
    "- Respond to user prompts\n",
    "- Decide which plugin functions to call\n",
    "- Use external knowledge sources like databases or APIs\n",
    "- Build better, grounded answers by combining model reasoning with real-world data\n",
    "\n",
    "You are about to connect powerful components:\n",
    "\n",
    "- Azure OpenAI (for embeddings and LLM chat completions)\n",
    "- PostgreSQL with Vector and Graph extensions (for fast semantic and graph search)\n",
    "- APIs for real-world data (historical weather evidence)\n",
    "\n",
    "As you progress, each section of code will incrementally build up these capabilities, and by the final step, you’ll have a highly capable legal research assistant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058ee98",
   "metadata": {},
   "source": [
    "### Part 3.2: Setup the Agent App Python imports\n",
    "\n",
    "> **Note:** In your lab environment, we already have the PIP packages pre-deployed that are needed by the import statements in the following code block, so you do not need to install these.  But just for reference and for future usage of this code, here are the packages used:\n",
    "> - PostgreSQL connectivity (`psycopg`, `psycopg-binary`, `psycopg-pool`)\n",
    "> - Modeling and validation (`pydantic`)\n",
    "> - OpenAI and Semantic Kernel integration (`openai`, `semantic-kernel`)\n",
    "> - Notebook compatibility (`nest_asyncio`, `ipykernel`)\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "This set of imports prepares the technical foundation for building an AI-powered agent that interacts with a PostgreSQL database and OpenAI services. `nest_asyncio` is used to allow nested event loops, which is important when running asynchronous code inside a Jupyter notebook. Core Python modules like `os`, `asyncio`, `uuid`, and `requests` handle environment access, asynchronous execution, unique ID generation, and external API calls, respectively. `psycopg2` provides database connectivity to PostgreSQL, while `pydantic` offers structured data validation and modeling.\n",
    "\n",
    "The Semantic Kernel libraries enable creation of intelligent agents (`ChatCompletionAgent`), define plugins and function tools (`kernel_function`), manage prompt settings, and handle retrieval-augmented memory through `PostgresMemoryStore` and `SemanticTextMemory`. Finally, Azure OpenAI integration components (`AzureChatCompletion`, `AzureTextEmbedding`) allow the agent to generate responses and embeddings using cloud-based AI models.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Run the cell below using the \"▶\" icon next to the cell.\n",
    "\n",
    "1. This will run the code and show the output below.  Since these are just imports, there is nothing to show at the end other than a check mark showing success.\n",
    "\n",
    "    > **Note:** The first time this code block is ran, it may take around 20-30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "844b751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import psycopg\n",
    "import uuid\n",
    "import requests\n",
    "from typing import Annotated\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.functions import kernel_function, KernelArguments\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding import AzureTextEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194337c",
   "metadata": {},
   "source": [
    "### Part 3.3: Setup environmental connection variables\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In order for our agent to interact with both Azure OpenAI services and the PostgreSQL database, we need to configure a few environment-specific connection variables. Rather than manually retrieving these values from the Azure Portal, we provide a shortcut: simply run the script located at `./Scripts/get_env.ps1` to automatically extract the correct credentials. After running the script, copy the output values into the fields below.\n",
    "\n",
    "This setup allows the agent to securely authenticate API requests to Azure OpenAI (using `AZURE_OPENAI_*` variables) and connect to the database where our case law data is stored (using the `DB_CONFIG` dictionary).\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Open the `get_env.ps1` file and update the `resourceGroupName` to match your lab environment. It should be like \"SKAgents-XXXXX\".\n",
    "1. Within VS Code, open a new terminal, and at the following path, enter:\n",
    "\n",
    "    `PS C:\\Users\\LabUser\\Downloads\\pg-sk-agents-lab> .\\Scripts\\get_env.ps1`\n",
    "\n",
    "1. From the output of the script in the terminal, copy the values for the following each into the variables in the code block below:\n",
    "    - `AZURE_OPENAI_ENDPOINT`\n",
    "    - `AZURE_OPENAI_KEY`\n",
    "    - `DB_CONFIG - HOST`\n",
    "    - `DB_CONFIG - PASSWORD`\n",
    "\n",
    "    > **Note:** For `DB_CONFIG - PASSWORD`, this is a very long string due to being an Entra ID Access Token, be sure to copy the entire string as the password.\n",
    "\n",
    "1. For the value for `{USER}`, this will be your Lab Username Credential, you can get these from the Lab Manual, in Part 3.3\n",
    "\n",
    "1. Once you have each of these fields filled in, then run the cell below using the \"▶\" icon next to the cell.  This will run the code and show the output below.  You will reuse these variables and objects throughout the lab notebook below.\n",
    "\n",
    "1. You should see a check mark when it completes, showing success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "56021df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_ENDPOINT   = \"\"\n",
    "AZURE_OPENAI_KEY        = \"\"\n",
    "AZURE_OPENAI_DEPLOYMENT = \"gpt-4o\"\n",
    "\n",
    "DB_CONFIG = {\n",
   "    \"host\":     \"\",\n",
    "    \"dbname\":   \"cases\",\n",
    "    \"user\":     \"\",\n",
    "    \"password\": \"\",\n",
    "    \"port\":     5432,\n",
    "    \"sslmode\":  \"require\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a3fd1",
   "metadata": {},
   "source": [
    "### Part 3.4: Create Semantic Kernel Plugin for Basic Database Queries\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this step, we create a custom plugin called DatabaseSearchPlugin to give our agent the ability to interact directly with the case law database using basic SQL queries. This plugin uses the psycopg2 library to establish a connection to PostgreSQL, execute queries, and return results. We define two simple but important functions: `count_cases()` to return the total number of cases in the database, and `search_cases(keyword)` to perform a keyword search against case opinions. Each function is decorated with `@kernel_function`, which registers it as a callable tool within the Semantic Kernel framework. This makes these database operations available to the agent automatically during conversation, enabling the agent to retrieve real-time, grounded information from our dataset.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review the code below, notice the print statements outputting to the terminal the name of the function when it is called.  This will be helpful later when we run the agent, and we want to see what functions it chose to call.\n",
    "\n",
    "1. Run the cell below using the \"▶\" icon next to the cell.\n",
    "\n",
    "1. This will run the code and show the output below.  Since these are just imports, there is nothing to show at the end other than a check mark showing success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "284bbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseSearchPlugin:\n",
    "        def __init__(self, cfg):\n",
    "            self.cfg = cfg\n",
    "\n",
    "        @kernel_function(description=\"Return the total number of cases in the database.\")\n",
    "        def count_cases(self) -> str:\n",
    "            \n",
    "            # print statement so we can see when this function chosen by the agent and is called\n",
    "            print(\"count_cases was called\")\n",
    "            \n",
    "            conn = psycopg.connect(**self.cfg)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"SELECT COUNT(*) FROM cases;\")\n",
    "            n = cur.fetchone()[0]\n",
    "            conn.close()\n",
    "            return str(n)\n",
    "\n",
    "        @kernel_function(description=\"Find up to 10 case IDs and names whose opinion contains the given keyword.\")\n",
    "        def search_cases(self, keyword: str) -> str:\n",
    "            \n",
    "            # print statement so we can see when this function chosen by the agent and is called\n",
    "            print(\"search_cases was called\")\n",
    "            \n",
    "            conn = psycopg.connect(**self.cfg)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\n",
    "                \"SELECT id, name, opinion FROM cases WHERE opinion ILIKE %s LIMIT 10;\",\n",
    "                (f\"%{keyword}%\",)\n",
    "            )\n",
    "            rows = cur.fetchall()\n",
    "            conn.close()\n",
    "            if not rows:\n",
    "                return \"No matches\"\n",
    "            return \"\\n\".join(f\"{r[0]}: {r[1]}: {r[2][:1000]}\" for r in rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0006bb",
   "metadata": {},
   "source": [
    "### Part 3.5: Test Run of our New Agent\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "Now that we have created our first plugin, we're ready to assemble and test an initial version of our agent. In this step, we create a default `OpenAIChatPromptExecutionSettings` to define basic settings. We then create an instance of `AzureChatCompletion`, which serves as the underlying communication layer between our agent and Azure OpenAI. Using these components, we instantiate a `ChatCompletionAgent`, providing it a name, a set of behavioral instructions, and a list of available plugins (in this case, just `DatabaseSearchPlugin`).\n",
    "\n",
    "Finally, we send a sample user query to the agent and retrieve its response. This test validates that our agent can successfully invoke plugin functions, query the database, and integrate the results into a natural language reply.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review the code below, notice the following:\n",
    "    - Inside `ChatCompletionAgent` we define `instructions` which act as notion of a \"system prompt\" for the Agent to define its purpose and goals\n",
    "    - For now we are passing in our `DatabaseSearchPlugin` class, we will be creating more PlugIns to enhance the functionality of our agent in the next steps\n",
    "    - Notice the `user_query` variable, and how it is asking how many cases there are, plus about water leaking cases.\n",
    "\n",
    "1. Run the cell below using the \"▶\" icon next to the cell.  This invokes the agent and subsequent LLM calls, this may take a few moments to run.\n",
    "\n",
    "1. After the code runs, notice the following:\n",
    "    - There should have been 2 functions called: `count_cases` and `search_cases`\n",
    "    - This happened because the agent interpreted the prompt and made a decision to call these 2 functions\n",
    "    - Notice how we got back a clear response of 377 cases are in our database.  This was based on our `count_cases` database function giving the LLM grounded truth about our dataset.\n",
    "    - Lastly, notice how we asked for 10 cases, but only got 2.  This is because our `search_cases` function is just the ILIKE operator and not yet using a vector search.  It could only find 2 cases that matching using the basic keyword ILIKE search.  In our next lab sections, we will see how we can improve on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a76e081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// Functions the Agent Called: //\n",
      "count_cases was called\n",
      "search_cases was called\n",
      "// Agent Response: //\n",
      "### Case Count:\n",
      "There are 377 cases in the database.\n",
      "\n",
      "### Cases Regarding Water Leaking:\n",
      "1. **Alexander v. Sanford**  \n",
      "   Reasoning: The case involves condominium owners suing developers and contractors for negligence that could involve water-related construction defects such as leaks.  \n",
      "   Opinion Summary: Claims were dismissed as untimely filed; cross-appealed attorney fees were denied for filing a frivolous lawsuit.\n",
      "\n",
      "2. **Warner v. Design & Build Homes, Inc.**  \n",
      "   Reasoning: The homeowners sued over structural defects and mold, which likely stemmed from water-related issues like leaks in construction.  \n",
      "   Opinion Summary: Claims were denied because implied warranties were disclaimed in an \"as-is\" sale and third-party beneficiary status was not established.\n",
      "\n",
      "Let me know if you need deeper insights into these opinions or additional cases!\n"
     ]
    }
   ],
   "source": [
    "settings = OpenAIChatPromptExecutionSettings()\n",
    "    \n",
    "chat_svc = AzureChatCompletion(\n",
    "    deployment_name=AZURE_OPENAI_DEPLOYMENT,\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_KEY)\n",
    "\n",
    "# Create agent with plugin and settings\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_svc,\n",
    "    name=\"SK-Assistant\",\n",
    "    instructions=\"You are a helpful legal assistant.  Respond to the user with the name of the cases and reasoning why the cases are the most relevant, and a short sentence summary of the opinion of the cases.\",\n",
    "    plugins=[DatabaseSearchPlugin(DB_CONFIG)],\n",
    "    arguments=KernelArguments(settings)\n",
    ")\n",
    "\n",
    "user_query = \"How many cases are there, and find me 10 cases regarding the notion of water leaking.\"\n",
    "\n",
    "print(\"// Functions the Agent Called: //\")\n",
    "\n",
    "response = await agent.get_response(messages=user_query)\n",
    "\n",
    "print(\"// Agent Response: //\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1206c174",
   "metadata": {},
   "source": [
    "### Part 3.6: Improve Agent Accuracy by Adding Semantic Re-ranking Query Plugin\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this step, we add a new plugin called `SemanticRerankingPlugin` to increase the precision of our agent’s search results. Instead of relying only on keyword matching, this plugin uses semantic search and re-ranking to evaluate results based on the meaning and intent of the user query.\n",
    "\n",
    "It works in two phases: first, it performs a vector similarity search using Azure OpenAI embeddings to find approximately relevant cases; then, it reorders these using a separate model (e.g., `semantic_relevance`) that scores and ranks the results for deeper semantic alignment. This two-step approach helps the agent prioritize results that are not just textually similar, but also topically and contextually relevant—making it ideal for legal queries where nuance matters. As with other tools, this function is registered using `@kernel_function`, so the agent can intelligently decide when to use it.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Before running this code block, we need to run a SQL script on the database.\n",
    "    - In VS Code, in the folder structure, in the folder `./Scripts/` navigate to the `setup_reranker.sql` file\n",
    "    - Once inside the file, still in VS Code, press on the keyboard `CTRL+SHIFT+P` to open the VS Code action panel, type `PGSQL Connect` and select the top `PGSQL: Connect` option\n",
    "    - Once prompted, select the Connection you made in the earlier steps in the lab\n",
    "    - You will then be asked the port number for the Connection, just hit `enter` to accept the default\n",
    "    - You should now be Connected to your database in the `setup_reranker.sql` file\n",
    "    - Next we need to replace the `{ENDPOINT}` and `{KEY}` tokens with OPenAI  Key and Endpoint\n",
    "    - In the editor window, click the \">\" button in the top right to run the script\n",
    "    - This will setup our connection to Azure ML via our `azure_ai` PostgreSQL database extension, and create 2 PostgresSQL PL/SQL functions needed for semantic re-ranking\n",
    "\n",
    "1. Review the code:\n",
    "    - Notice the following enhancements:\n",
    "        - This plugin introduces a function called `search_semantic_reranked_cases`, designed to deliver higher-quality search results by understanding the semantic meaning of the query rather than relying on basic keyword matches.\n",
    "        - The function is decorated with `@kernel_function`, which makes it available to the agent as a callable tool-this is how Semantic Kernel enables function calling automatically based on user intent.\n",
    "\n",
    "    - Check out the SQL logic inside the code, notice how it includes two key phases:\n",
    "        - (Phase 1) A vector similarity search using OpenAI-generated embeddings to find top 60 candidate cases.\n",
    "        - (Phase 2) A semantic re-ranking step that uses an external re-ranker model (semantic_relevance) to evaluate and reorder those 60 based on relevance scores.\n",
    "        - This hybrid approach helps ensure that even when the user’s phrasing doesn’t exactly match the database text, relevant results can still be surfaced based on meaning and context.\n",
    "\n",
    "    - Observe how this plugin builds on the earlier keyword search:\n",
    "        - The previous `search_cases()` method used `ILIKE` for fuzzy keyword matching. That works for exact or near-exact phrases but misses nuance.\n",
    "        - This plugin improves accuracy and recall, especially for complex queries or abstract legal concepts where keyword overlap may be weak.\n",
    "        - This function will be automatically called by the agent if the prompt includes phrases like \"high accuracy is important\" or contains complex, open-ended search intent.\n",
    "        - For example, a prompt like: \"*Help me find the most relevant cases about tenant water damage, with high accuracy*\" will likely trigger this plugin over the basic one.\n",
    "\n",
    "1. Finally, run the code block cell by clicking the \"▶\" button on the left side of the code block.\n",
    "    - This will run the code but there will be no output yet.\n",
    "    - Since this is just a class definition, there is nothing to show at the end other than a check mark showing success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fa89ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the required library is installed\n",
    "!pip install -q sentence-transformers\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "import psycopg\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "class SemanticRerankingPlugin:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        # Load CrossEncoder model (you can choose a more accurate one if needed)\n",
    "        self.model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    @kernel_function(description=\"Use semantic re-ranking function to query and find cases matching the query based on semantic intent and relevance. Use this function when high accuracy is needed.\")\n",
    "    def search_semantic_reranked_cases(self, query: str) -> str:\n",
    "        print(\"search_semantic_reranked_cases was called\")\n",
    "\n",
    "        conn = psycopg.connect(**self.cfg)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Step 1–2: Vector Search to fetch top 60 candidate cases\n",
    "        cur.execute(\"\"\"\n",
    "            WITH embedding_query AS (\n",
    "                SELECT azure_openai.create_embeddings('text-embedding-3-small', %s)::vector AS embedding\n",
    "            )\n",
    "            SELECT id, name, opinion\n",
    "            FROM cases, embedding_query\n",
    "            ORDER BY opinions_vector <=> embedding\n",
    "            LIMIT 60;\n",
    "        \"\"\", (query,))\n",
    "        rows = cur.fetchall()\n",
    "        conn.close()\n",
    "\n",
    "        if not rows:\n",
    "            return \"No matches found.\"\n",
    "\n",
    "        # Step 3: Rerank using CrossEncoder\n",
    "        case_texts = [f\"{r[1]}: {r[2]}\" for r in rows]\n",
    "        pairs = [(query, text) for text in case_texts]\n",
    "        scores = self.model.predict(pairs)\n",
    "\n",
    "        # Step 4: Sort by score\n",
    "        reranked = sorted(zip(rows, scores), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        # ✅ Step 5: Format results safely\n",
    "        return \"\\n\".join(\n",
    "            f\"{row[0]}: {row[1]} (score: {round(score, 3)}): {row[2][:500]}...\"\n",
    "            for row, score in reranked\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ef674482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRerankingPlugin:\n",
    "        def __init__(self, cfg):\n",
    "            # Store the database configuration for later use\n",
    "            self.cfg = cfg\n",
    "\n",
    "        @kernel_function(description=\"Use semantic re-ranking function to query and find cases matching the query based on semantic intent and relevance.  Use this function when high accuracy is needed.\")\n",
    "        def search_semantic_reranked_cases(self, query: str) -> str:\n",
    "            \n",
    "            # Log when the function is invoked\n",
    "            print(\"search_semantic_reranked_cases was called\")\n",
    "            \n",
    "            # Connect to the PostgreSQL database\n",
    "            conn = psycopg.connect(**self.cfg)\n",
    "            cur = conn.cursor()\n",
    "\n",
    "            # Execute a SQL query that performs semantic vector search and re-ranking\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                -- // Step 1: Create an embedding from the user's query using Azure OpenAI //\n",
    "                WITH embedding_query AS (\n",
    "                    SELECT azure_openai.create_embeddings('text-embedding-3-small', %s)::vector AS embedding\n",
    "                ),\n",
    "\n",
    "                -- // Step 2: Find top 60 cases whose vector is closest to the query embedding (initial vector search) //\n",
    "                vector AS (\n",
    "                    SELECT cases.id as case_id, cases.name AS case_name, cases.opinion, RANK() OVER (ORDER BY opinions_vector <=> embedding) AS vector_rank\n",
    "                    FROM cases, embedding_query\n",
    "                    ORDER BY opinions_vector <=> embedding\n",
    "                    LIMIT 60\n",
    "                ),\n",
    "\n",
    "                -- // Step 3: Call a semantic re-ranking function to evaluate the 60 candidates and assign relevance scores //\n",
    "                semantic AS (\n",
    "                    SELECT * \n",
    "                    FROM jsonb_array_elements(\n",
    "                            semantic_relevance(%s, 60)\n",
    "                        ) WITH ORDINALITY AS elem(relevance)\n",
    "                ),\n",
    "\n",
    "                -- // Step 4: Join the initial vector results with semantic scores and rank by highest relevance //\n",
    "                semantic_ranked AS (\n",
    "                    SELECT semantic.relevance::DOUBLE PRECISION AS relevance, RANK() OVER (ORDER BY relevance DESC) AS semantic_rank,\n",
    "                            semantic.*, vector.*\n",
    "                    FROM vector\n",
    "                    JOIN semantic ON vector.vector_rank = semantic.ordinality\n",
    "                    ORDER BY semantic.relevance DESC\n",
    "                )\n",
    "\n",
    "                -- // Step 5: Return the top 10 results after semantic re-ranking //\n",
    "                SELECT case_id, case_name, opinion\n",
    "                FROM semantic_ranked\n",
    "                LIMIT 10;\n",
    "                \"\"\", (query, query)) # Pass the query twice: once for embedding, once for semantic re-ranker\n",
    "\n",
    "            rows = cur.fetchall() # Fetch the final ranked results\n",
    "\n",
    "            conn.close() # Always close the connection after use\n",
    "\n",
    "            # Return a nicely formatted list of top results or fallback message if no matches\n",
    "            if not rows:\n",
    "                return \"No matches\"\n",
    "            return \"\\n\".join(f\"{r[0]}: {r[1]}: {r[2][:1000]}\" for r in rows) # Truncate long opinions to 1000 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1491f",
   "metadata": {},
   "source": [
    "### Part 3.7: Add a GraphRAG Query PlugIn to the Agent for Additional Accuracy Improvements\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this step, we build another advanced plugin, `GraphDatabasePlugin`, that combines vector search with graph analysis to find the most influential cases related to a query topic. The `search_graph_cases` method first uses an embedding-based similarity search to semantically rank cases from the cases table. This ensures that only cases meaningfully related to the query are considered further.\n",
    "\n",
    "After narrowing the results semantically, the query leverages **Apache AGE** - a PostgreSQL extension that enables property graph querying via Cypher syntax. Specifically, it matches citations (relationships between cases) in the `case_graph` graph. By counting the number of incoming edges (citations) for each case, we can rank cases based on their influence or importance within the graph. Cases with more citations are prioritized, resulting in a more nuanced retrieval that considers both semantic relevance and citation authority.\n",
    "\n",
    "This hybrid retrieval technique is an example of **GraphRAG (Graph Retrieval-Augmented Generation)** and represents a more sophisticated form of grounded information retrieval for legal, academic, or research applications.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Before running this code block, we need to run a SQL script on the database.\n",
    "    - In VS Code, in the folder structure, in the folder `./Scripts/` navigate to the `create_graph.sql` file\n",
    "    - Once inside the file, still in VS Code, press on the keyboard `CTRL+SHIFT+P` to open the VS Code action panel, type `PGSQL Connect` and select the top `PGSQL: Connect` option\n",
    "    - Once prompted, select the Connection you made in the earlier steps in the lab\n",
    "    - You will then be asked the port number for the Connection, just hit `enter` to accept the default\n",
    "    - You should now be Connected to your database in the `create_graph.sql` file\n",
    "    - In the editor window, click the \">\" button in the top right to run the script\n",
    "    - This will build our graph database via the Apache AGE extension in our Azure PostgreSQL database using our loaded 377 legal cases\n",
    "\n",
    "1. Next, because we are using the Apache AGE PostgreSQL extension to provide us Graph database capabilities, we need to enable the extension on our database.\n",
    "    - Run the following PowerShell script within VS Code\n",
    "    - Within VS Code, open a new terminal, and at the following path, enter:\n",
    "\n",
    "        `PS C:\\Users\\LabUser\\Downloads\\pg-sk-agents-lab> .\\Scripts\\load_age.ps1`\n",
    "\n",
    "        > **Note:** This will run through 3 main commands, all together will take around 60-120 seconds.\n",
    "\n",
    "1. Review the code:\n",
    "    - This plugin allows the agent to find legal cases that are not only semantically similar to a user's query but also highly cited by other cases - providing both relevance and legal importance. The `@kernel_function` decorator makes this method callable by the agent.\n",
    "    - Look at the `semantic_ranked` CTE (Common Table Expression). This ranks cases by their similarity to the input query using embedding-based vector comparison (<=>). The function limits results to the top 60 most semantically similar opinions using Azure OpenAI’s embedding model.\n",
    "    - Examine the graph CTE. It runs a Cypher query on the `case_graph` to count how many times each case is cited by others. These citation counts are joined to the semantic results using the case ID. This allows the plugin to prioritize not just relevant cases, but those that are also influential in the citation network.\n",
    "    - The final `SELECT` returns 10 cases with the highest number of citations among the semantically relevant ones. The list is ordered by refs `DESC`, meaning more citations come first. Each opinion is truncated to the first 1000 characters to keep responses concise.\n",
    "\n",
    "1. Finally, run the code block cell by clicking the \"▶\" button on the left side of the code block.\n",
    "    - This will run the code but there will be no output yet.\n",
    "    - Since this is again just a class definition, there is nothing to show at the end other than a check mark showing success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9d2ec57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDatabasePlugin:\n",
    "        def __init__(self, cfg):\n",
    "            # Store the database configuration dictionary\n",
    "            self.cfg = cfg\n",
    "\n",
    "        @kernel_function(description=\"Use an advanced accuracy query to find important cases with high levels of citations about the query topic.\")\n",
    "        def search_graph_cases(self, query: str) -> str:\n",
    "            \n",
    "            # Debug log to indicate function was triggered\n",
    "            print(\"search_graph_cases was called\")\n",
    "            \n",
    "            # Connect to the PostgreSQL database using the provided config\n",
    "            conn = psycopg.connect(**self.cfg)\n",
    "            cur = conn.cursor()\n",
    "\n",
    "            # Set the search path first\n",
    "            # Step 1: Set the schema search path to include Apache AGE graph catalog\n",
    "            cur.execute('SET search_path = public, ag_catalog, \"$user\";')\n",
    "\n",
    "            # Execute a compound query using semantic search + graph analysis (Apache AGE)\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                -- // Step 2: Rank cases semantically using embedding similarity //\n",
    "                WITH semantic_ranked AS (\n",
    "                    SELECT id, name, opinion, opinions_vector\n",
    "                    FROM cases\n",
    "                    ORDER BY opinions_vector <=> azure_openai.create_embeddings('text-embedding-3-small', %s)::vector\n",
    "                    LIMIT 60\n",
    "                ),\n",
    "                -- // Step 3: Use a Cypher graph query to count how many citations (edges) each case has //\n",
    "                graph AS (\n",
    "                    SELECT graph_query.refs, semantic_ranked.*, graph_query.case_id \n",
    "                    FROM semantic_ranked\n",
    "                    LEFT JOIN cypher('case_graph', $$\n",
    "                        MATCH ()-[r]->(n)\n",
    "                        RETURN n.case_id, COUNT(r) AS refs\n",
    "                    $$) as graph_query(case_id TEXT, refs BIGINT)\n",
    "                    ON semantic_ranked.id = graph_query.case_id::int\n",
    "                )\n",
    "                -- // Step 4: Return the top 10 cases, prioritized by number of citations (refs) //\n",
    "                SELECT id, name, opinion\n",
    "                FROM graph\n",
    "                ORDER BY refs DESC NULLS LAST\n",
    "                LIMIT 10;\n",
    "                \"\"\", \n",
    "                (query,)\n",
    "            )\n",
    "            rows = cur.fetchall() # Fetch all resulting rows\n",
    "            conn.close() # Close the database connection\n",
    "\n",
    "            # Return either \"No matches\" or a formatted list of results\n",
    "            if not rows:\n",
    "                return \"No matches\"\n",
    "            return \"\\n\".join(f\"{r[0]}: {r[1]}: {r[2][:1000]}\" for r in rows) # Truncate long opinions to 1000 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3164fd",
   "metadata": {},
   "source": [
    "### Part 3.8: Re-Assemble our Agent with new advanced PlugIns and Re-Test\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this step, we re-assemble the full agent by attaching all of the custom plugins we’ve created so far: `DatabaseSearchPlugin`, `SemanticRerankingPlugin`, and `GraphDatabasePlugin`. These plugins give the agent access to different querying strategies, from simple keyword searches to advanced semantic filtering and graph-based citation analysis. By registering all plugins in the plugins list, we enable the agent to choose the right tool based on the intent expressed in the user’s prompt.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review the code below, notice the following:\n",
    "    - We only need to re-define our agent object using the `ChatCompletionAgent` class\n",
    "    - Notice we are now adding our new PlugIns on the line:\n",
    "        - `plugins=[DatabaseSearchPlugin(DB_CONFIG),SemanticRerankingPlugin(DB_CONFIG),GraphDatabasePlugin(DB_CONFIG)],`    \n",
    "    - Notice the `user_query` variables, there are some additional ones you can try testing yourself by uncommenting one at a time, then re-running the code cell\n",
    "\n",
    "1. Run the cell below using the \">\" icon next to the cell.  This invokes the agent and subsequent LLM calls, this may take a few moments to run.\n",
    "\n",
    "1. After the code runs, notice the following:\n",
    "    - Depending on the prompt you chosen, there should have been between 2-3 functions called, such as: `search_graph_cases` and `search_semantic_reranked_cases`\n",
    "    - Notice how we asked for 10 cases, and now got 10 cases. This is because the we are now using semantic vector search, not just keyword search for the database queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "51df009d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// Functions the Agent Called: //\n",
      "search_semantic_reranked_cases was called\n",
      "search_graph_cases was called\n",
      "count_cases was called\n",
      "// Agent Response: //\n",
      "### Total Number of Cases\n",
      "The database contains a very large number of cases related to various legal matters.\n",
      "\n",
      "### Relevant Cases (with Summary)\n",
      "Here are some of the most relevant cases connected to water leak issues in apartments:\n",
      "\n",
      "1. **Wilkening v. Watkins Distributors, Inc.**\n",
      "   - Reasoning: Details damages involving the obligations of lessees and assignees under property rental agreements.\n",
      "   - Opinion Summary: The court held the lessee liable for repairs, affirming the trial's decision.\n",
      "\n",
      "2. **Laurelon Terrace, Inc. v. City of Seattle**\n",
      "   - Reasoning: Involves flooding and water damage liability, showcasing the nuances of contributory negligence.\n",
      "   - Opinion Summary: The order for a new trial emphasized contributory negligence instructions.\n",
      "\n",
      "3. **Stevens v. King County**\n",
      "   - Reasoning: Discusses liability for property damage caused by water flow and invasion of surface water.\n",
      "   - Opinion Summary: The court clarified negligence issues and upheld the city's liability for damages.\n",
      "\n",
      "4. **Westlake View Condominium Association v. Sixth Avenue View Partners, LLC**\n",
      "   - Reasoning: Focuses on water intrusion into condominium units and the builder's implied warranty of habitability.\n",
      "   - Opinion Summary: Summary judgment for the builder was reversed as evidence raised material fact questions.\n",
      "\n",
      "5. **Teglo v. Porter**\n",
      "   - Reasoning: Examines landlord responsibility when leased premises suffer from a structural defect causing tenant injury.\n",
      "   - Opinion Summary: The court dismissed the landlord's liability based on insufficient covenant evidence.\n",
      "\n",
      "6. **Papac v. City of Montesano**\n",
      "   - Reasoning: Concerns flooding attributed to public projects impacting adjacent properties.\n",
      "   - Opinion Summary: The city was held responsible for nuisance damages to private property.\n",
      "\n",
      "7. **Foisy v. Wyman**\n",
      "   - Reasoning: Legal principles surrounding unpaid rent and tenants' continued occupation post-lease duration.\n",
      "   - Opinion Summary: The plaintiff landlord's claims for damages against tenant default were upheld.\n",
      "\n",
      "8. **Strickland v. City of Seattle**\n",
      "   - Reasoning: Handles damage claims over city actions causing sediment and water accumulation affecting private homes.\n",
      "   - Opinion Summary: The city's actions were deemed compatible with public duties, not unjustly damaging.\n",
      "\n",
      "9. **Hughes v. King County**\n",
      "   - Reasoning: Addresses storm sewer overflow causing property damage, establishing county liability.\n",
      "   - Opinion Summary: The court ruled in favor of homeowners for damages incurred from sewer system overflow.\n",
      "\n",
      "10. **1515-1519 Lakeview Boulevard Condominium Association v. Apartment Sales Corp.**\n",
      "    - Reasoning: Discusses negligence by the city and exculpatory clauses in land-use approvals affecting property stability.\n",
      "    - Opinion Summary: The case brought attention to conditions of negligence and legal clause enforceability.\n",
      "\n",
      "These cases help highlight potential legal outcomes regarding water leaks and responsibilities of landlords, tenants, adjacent property owners, and governing authorities.\n"
     ]
    }
   ],
   "source": [
    "# Re-Create agent with new plugins\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_svc,\n",
    "    name=\"SK-Assistant\",\n",
    "    instructions=\"You are a helpful legal assistant.  Respond to the user with the name of the cases and reasoning why the cases are the most relevant, and a short sentence summary of the opinion of the cases.\",\n",
    "    plugins=[DatabaseSearchPlugin(DB_CONFIG),SemanticRerankingPlugin(DB_CONFIG),GraphDatabasePlugin(DB_CONFIG)],\n",
    "    arguments=KernelArguments(settings)\n",
    ")\n",
    "\n",
    "# Try these different prompts to see how the agent responds:\n",
    "# Remove one comment at a time to test different prompts\n",
    "user_query = \"Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.  Also how many cases are there overall?\"\n",
    "#user_query = \"Bring into 1 list of 10 cases, ranked by relevancy -- Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.\"\n",
    "#user_query = \"How many cases are there, and high accuracy is important, help me find 10 highly relevant cases related to water leaking in my apartment.\"\n",
    "\n",
    "print(\"// Functions the Agent Called: //\")\n",
    "\n",
    "response = await agent.get_response(messages=user_query)\n",
    "\n",
    "print(\"// Agent Response: //\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32517a",
   "metadata": {},
   "source": [
    "### Part 3.9: Adding a Weather PlugIn to the Agent\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "To enhance our legal assistant with real-world context, we introduce a `WeatherPlugin` that enables the agent to retrieve historical weather data (specifically rainfall) based on a given date and geographic location. This is especially useful in real estate or tenant-landlord disputes where weather-related damage (e.g., leaks or flooding) may be a legal factor. The plugin uses the Open-Meteo Archive API, a free and reliable weather data service.\n",
    "\n",
    "When a user prompt mentions a need for weather data—such as \"What was the rainfall on Feb 1, 2024, in Seattle?\"—the agent will automatically call this function. The returned data provides accurate, grounded evidence that enhances the agent’s response and credibility.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review the code below and observe the following:\n",
    "    - The `@kernel_function` decorator registers this function so the agent can call it based on user intent.\n",
    "    - The plugin uses `requests.get()` to make a live API call to Open-Meteo, passing latitude, longitude, and date parameters.\n",
    "    - The response is parsed from JSON and extracts the precipitation value from the `daily.precipitation_sum` array.\n",
    "    - The result is returned as a readable string with the date, coordinates, and rainfall in millimeters.\n",
    "\n",
    "1. Run the cell below using the \"▶\" (Run) button. There is no visible output until the function is called by the agent in a relevant prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bf9190f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherPlugin:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Get total precipitation (in mm) on a given date and location (latitude, longitude).\"\n",
    "    )\n",
    "    def get_historical_rainfall(self, date: str, latitude: float, longitude: float) -> str:\n",
    "        \"\"\"\n",
    "        date: YYYY-MM-DD\n",
    "        latitude, longitude: WGS84 coords\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"get_historical_rainfall was called\")\n",
    "\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"start_date\": date,\n",
    "            \"end_date\":   date,\n",
    "            \"daily\":      \"precipitation_sum\",\n",
    "            \"timezone\":   \"UTC\"\n",
    "        }\n",
    "        resp = requests.get(url, params=params)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        data = resp.json()\n",
    "        # the API returns a list for daily.precipitation_sum\n",
    "        rain_mm = data[\"daily\"][\"precipitation_sum\"][0]\n",
    "        return f\"On {date} at ({latitude}, {longitude}), total precipitation was {rain_mm} mm.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613a5df",
   "metadata": {},
   "source": [
    "### Part 3.10: Add our new Weather PlugIn to our Agent and Re-Test\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this step, we complete our agent by including the new `WeatherPlugin` alongside our database and semantic plugins. This enables the agent to answer more complex, multi-part prompts that require both legal case analysis and external factual grounding—such as historical rainfall on a specific date and location.\n",
    "\n",
    "When a user prompt mentions weather-related conditions (e.g., \"*What was the rainfall on February 1, 2024, in Seattle?*\"), the agent can automatically call the appropriate plugin function. Semantic Kernel handles function selection based on the natural language intent, so all tools are available simultaneously without manual switching. This demonstrates the power of multi-plugin orchestration in agent design—allowing a single agent to reason across legal data and real-world APIs in one coherent response.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review how the `WeatherPlugin()` is added to the list of available plugins in the `ChatCompletionAgent`.\n",
    "\n",
    "1. Examine the user_query. Note how it includes both:\n",
    "    - A factual request (weather evidence),\n",
    "    - And a legal research task (finding relevant legal cases).\n",
    "\n",
    "1. Run the cell below using the \"▶\" (Run) button and observe:\n",
    "    - Which functions the agent chooses to call (check your logs or printed output),\n",
    "    - How the agent combines different results into a single response.\n",
    "    - Try editing the user_query to include other dates or cities and observe how the weather data changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b672c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// Functions the Agent Called: //\n",
      "get_historical_rainfall was called\n",
      "search_semantic_reranked_cases was called\n",
      "// Agent Response: //\n",
      "### Historical Rainfall\n",
      "On February 1, 2024, the total precipitation in Seattle, WA was recorded as 5.1 mm.\n",
      "\n",
      "### Highly Relevant Cases on Water Leaking in Apartments\n",
      "1. **Wilkening v. Watkins Distributors, Inc.**: Concerns damages due to structural issues and lessor-liability limits under a lease agreement. Affirmed judgment on roof repairs.\n",
      "2. **Frisken v. Art Strand Floor Coverings, Inc.**: Evaluates liability for improper flooring installation causing leaks. The installation was deemed defective.\n",
      "3. **Buttnick v. Clothier**: Explores breach of lease terms and affirmative defenses in a landlord-tenant dispute over property conditions.\n",
      "4. **Laurelon Terrace, Inc. v. City of Seattle**: Reviews damages and negligence in flooding caused by municipal actions.\n",
      "5. **Schedler v. Wagner**: Addresses tenant injuries due to icy conditions in common areas maintained poorly by landlords.\n",
      "6. **Stevens v. King County**: Discusses city liability for damages caused by surface water invasion onto private property.\n",
      "7. **Puget Investment Co. v. Wenck**: Evaluates tenant liability for repair obligations and compliance with property use conditions.\n",
      "8. **Pham v. Corbett**: Focuses on breach of the implied warranty of habitability with water intrusion claims by tenants.\n",
      "9. **Westlake View Condominium Ass'n v. Sixth Avenue View Partners, LLC**: Reviews implied warranty of habitability claims due to water intrusion and other structural defects.\n",
      "10. **Pappas v. Zerwoodis**: Explores lessee remedies for delayed repairs by landlords causing property issues.\n",
      "\n",
      "These cases are relevant as they address landlord-tenant disputes related to water damage, habitability, repair obligations, and structural defects.\n"
     ]
    }
   ],
   "source": [
    "# Re-Create agent with new plugins\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_svc,\n",
    "    name=\"SK-Assistant\",\n",
    "    instructions=\"You are a helpful legal assistant.  Respond to the user with the name of the cases and reasoning why the cases are the most relevant, and a short sentence summary of the opinion of the cases.\",\n",
    "    plugins=[DatabaseSearchPlugin(DB_CONFIG),SemanticRerankingPlugin(DB_CONFIG),GraphDatabasePlugin(DB_CONFIG),WeatherPlugin()],\n",
    "    arguments=KernelArguments(settings)\n",
    ")\n",
    "\n",
    "# Try these different prompts to see how the agent responds:\n",
    "# Remove one comment at a time to test different prompts\n",
    "user_query = \"What was the rainfall on 2024-02-01 in Seattle, WA? I need this for evidence.  Also High accuracy is important, help me find 10 highly relevant cases related to water leaking in my clients apartment.\"\n",
    "#user_query = \"I am a real estate lawyer, so cases need to be related to real estate law.\"\n",
    "#user_query = \"Find me 10 cases regarding the notion of water leaking.\"\n",
    "#user_query = \"How many cases are there, and high accuracy is important, help me find 10 highly relevant cases related to water leaking in my apartment.\"\n",
    "#user_query = \"Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.  Also how many cases are there overall?\"\n",
    "#user_query = \"Bring into 1 list of 10 cases, ranked by relevancy -- Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above.  High accuracy is important, and high number of citations is important.\"\n",
    "#user_query = \"How many cases are in my database? What was the rainfall on 2024-02-01 in Seattle, WA? I need this for evidence.  Also High accuracy is important, help me find 10 highly relevant cases related to water leaking in my clients apartment.\"\n",
    "\n",
    "print(\"// Functions the Agent Called: //\")\n",
    "response = await agent.get_response(messages=user_query)\n",
    "\n",
    "print(\"// Agent Response: //\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784b47b",
   "metadata": {},
   "source": [
    "### Part 3.11: Adding Memory into the Agent\n",
    "\n",
    "##### 🧠 *Technical Notes*\n",
    "\n",
    "In this final step, we complete our agent’s capabilities by enabling semantic memory using SemanticTextMemory backed by a PostgreSQL vector store. This allows the agent to store and recall relevant prior user queries and its own responses—creating a more personalized and context-aware experience over time.\n",
    "\n",
    "When a new prompt is received, the agent retrieves semantically similar past queries from memory using embedding-based vector search. These memories are then prepended to the current prompt, giving the agent important context and continuity across interactions. This memory is especially useful in real-world legal scenarios where a user may build a case over several prompts, and the agent must retain prior details to offer more precise, relevant guidance.\n",
    "\n",
    "##### ⚙️ *Code Review Tasks*\n",
    "\n",
    "1. Review how PostgresMemoryStore and AzureTextEmbedding are used to set up vector-based memory for the agent.\n",
    "\n",
    "1. Examine how the user query is stored into memory, and how top related queries are retrieved with semantic search.\n",
    "\n",
    "1. Observe how the memory context is prepended to the new prompt before being passed to the agent.\n",
    "\n",
    "1. Run the cell below using the \"▶\" (Run) button and inspect:\n",
    "    - How the agent’s output incorporates the memory context,\n",
    "    - What gets saved to memory as both the query and the agent’s reply,\n",
    "    - How this memory layer enables continuity in multi-turn conversations.\n",
    "\n",
    "1. Test running through the first 3 sample prompts below, by uncommenting a single prompt, one at a time\n",
    "    - As you run through these, this helps to showcase the memory functionality as the Agent keeps track of these facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "48f4ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: semantic-kernel 1.34.0\n",
      "Uninstalling semantic-kernel-1.34.0:\n",
      "  Successfully uninstalled semantic-kernel-1.34.0\n",
      "Collecting git+https://github.com/microsoft/semantic-kernel.git@main#subdirectory=python\n",
      "  Cloning https://github.com/microsoft/semantic-kernel.git (to revision main) to c:\\users\\demouser\\appdata\\local\\temp\\2\\pip-req-build-mqmzpxdt\n",
      "  Resolved https://github.com/microsoft/semantic-kernel.git to commit ba000c3368f34ced81c899b85bb519545e344ff8\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: azure-ai-projects>=1.0.0b11 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (1.0.0b12)\n",
      "Requirement already satisfied: azure-ai-agents>=1.1.0b1 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (1.1.0b2)\n",
      "Requirement already satisfied: aiohttp~=3.8 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (3.12.13)\n",
      "Requirement already satisfied: cloudevents~=1.0 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (1.12.0)\n",
      "Requirement already satisfied: pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (2.11.7)\n",
      "Requirement already satisfied: pydantic-settings~=2.0 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (2.10.1)\n",
      "Requirement already satisfied: defusedxml~=0.7 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (0.7.1)\n",
      "Requirement already satisfied: azure-identity>=1.13 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (1.23.0)\n",
      "Requirement already satisfied: numpy>=1.25.0 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.67 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (1.93.0)\n",
      "Requirement already satisfied: openapi_core<0.20,>=0.18 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (0.18.2)\n",
      "Requirement already satisfied: websockets<16,>=13 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (15.0.1)\n",
      "Requirement already satisfied: aiortc>=1.9.0 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (1.13.0)\n",
      "Requirement already satisfied: opentelemetry-api~=1.24 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.24 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (1.34.1)\n",
      "Requirement already satisfied: prance<25.4.9,>=23.6.21 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (23.6.21.0)\n",
      "Requirement already satisfied: pybars4~=0.9 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (0.9.13)\n",
      "Requirement already satisfied: jinja2~=3.1 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (3.1.6)\n",
      "Requirement already satisfied: nest-asyncio~=1.6 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.15.1 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (1.15.3)\n",
      "Requirement already satisfied: protobuf in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (6.31.1)\n",
      "Requirement already satisfied: typing-extensions>=4.13 in c:\\python310\\lib\\site-packages (from semantic-kernel==1.34.0) (4.14.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\python310\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel==1.34.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python310\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel==1.34.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\python310\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel==1.34.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python310\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel==1.34.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python310\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel==1.34.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python310\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel==1.34.0) (6.6.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\python310\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel==1.34.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\python310\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel==1.34.0) (1.20.1)\n",
      "Requirement already satisfied: deprecation<3.0,>=2.0 in c:\\python310\\lib\\site-packages (from cloudevents~=1.0->semantic-kernel==1.34.0) (2.1.0)\n",
      "Requirement already satisfied: packaging in c:\\python310\\lib\\site-packages (from deprecation<3.0,>=2.0->cloudevents~=1.0->semantic-kernel==1.34.0) (25.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2~=3.1->semantic-kernel==1.34.0) (3.0.2)\n",
      "Requirement already satisfied: asgiref<4.0.0,>=3.6.0 in c:\\python310\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (3.8.1)\n",
      "Requirement already satisfied: isodate in c:\\python310\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (0.7.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in c:\\python310\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (4.24.0)\n",
      "Requirement already satisfied: jsonschema-spec<0.3.0,>=0.2.3 in c:\\python310\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (0.2.4)\n",
      "Requirement already satisfied: more-itertools in c:\\python310\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (10.7.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in c:\\python310\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (0.6.3)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in c:\\python310\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (0.7.2)\n",
      "Requirement already satisfied: parse in c:\\python310\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (1.20.2)\n",
      "Requirement already satisfied: werkzeug in c:\\python310\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (3.1.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\python310\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\python310\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\python310\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (0.25.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\python310\\lib\\site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (6.0.2)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in c:\\python310\\lib\\site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (0.4.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\python310\\lib\\site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (2.32.4)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\python310\\lib\\site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (0.1.4)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in c:\\python310\\lib\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (0.3.4)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in c:\\python310\\lib\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (1.11.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\python310\\lib\\site-packages (from opentelemetry-api~=1.24->semantic-kernel==1.34.0) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\python310\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api~=1.24->semantic-kernel==1.34.0) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in c:\\python310\\lib\\site-packages (from opentelemetry-sdk~=1.24->semantic-kernel==1.34.0) (0.55b1)\n",
      "Requirement already satisfied: chardet>=3.0 in c:\\python310\\lib\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel==1.34.0) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in c:\\python310\\lib\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel==1.34.0) (0.18.14)\n",
      "Requirement already satisfied: six~=1.15 in c:\\python310\\lib\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel==1.34.0) (1.17.0)\n",
      "Requirement already satisfied: PyMeta3>=0.5.1 in c:\\python310\\lib\\site-packages (from pybars4~=0.9->semantic-kernel==1.34.0) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python310\\lib\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel==1.34.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\python310\\lib\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel==1.34.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\python310\\lib\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel==1.34.0) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\python310\\lib\\site-packages (from pydantic-settings~=2.0->semantic-kernel==1.34.0) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.31.0->jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.31.0->jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.31.0->jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.31.0->jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.20,>=0.18->semantic-kernel==1.34.0) (2025.6.15)\n",
      "Requirement already satisfied: aioice<1.0.0,>=0.10.1 in c:\\python310\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel==1.34.0) (0.10.1)\n",
      "Requirement already satisfied: av<15.0.0,>=14.0.0 in c:\\python310\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel==1.34.0) (14.4.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\python310\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel==1.34.0) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=44.0.0 in c:\\python310\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel==1.34.0) (45.0.4)\n",
      "Requirement already satisfied: google-crc32c>=1.1 in c:\\python310\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel==1.34.0) (1.7.1)\n",
      "Requirement already satisfied: pyee>=13.0.0 in c:\\python310\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel==1.34.0) (13.0.0)\n",
      "Requirement already satisfied: pylibsrtp>=0.10.0 in c:\\python310\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel==1.34.0) (0.12.0)\n",
      "Requirement already satisfied: pyopenssl>=25.0.0 in c:\\python310\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel==1.34.0) (25.1.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\python310\\lib\\site-packages (from aioice<1.0.0,>=0.10.1->aiortc>=1.9.0->semantic-kernel==1.34.0) (2.7.0)\n",
      "Requirement already satisfied: ifaddr>=0.2.0 in c:\\python310\\lib\\site-packages (from aioice<1.0.0,>=0.10.1->aiortc>=1.9.0->semantic-kernel==1.34.0) (0.2.0)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in c:\\python310\\lib\\site-packages (from azure-ai-agents>=1.1.0b1->semantic-kernel==1.34.0) (1.34.0)\n",
      "Requirement already satisfied: azure-storage-blob>=12.15.0 in c:\\python310\\lib\\site-packages (from azure-ai-projects>=1.0.0b11->semantic-kernel==1.34.0) (12.25.1)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\python310\\lib\\site-packages (from azure-identity>=1.13->semantic-kernel==1.34.0) (1.32.3)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\python310\\lib\\site-packages (from azure-identity>=1.13->semantic-kernel==1.34.0) (1.3.1)\n",
      "Requirement already satisfied: pycparser in c:\\python310\\lib\\site-packages (from cffi>=1.0.0->aiortc>=1.9.0->semantic-kernel==1.34.0) (2.22)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\python310\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity>=1.13->semantic-kernel==1.34.0) (2.10.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python310\\lib\\site-packages (from openai>=1.67->semantic-kernel==1.34.0) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python310\\lib\\site-packages (from openai>=1.67->semantic-kernel==1.34.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python310\\lib\\site-packages (from openai>=1.67->semantic-kernel==1.34.0) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\python310\\lib\\site-packages (from openai>=1.67->semantic-kernel==1.34.0) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\python310\\lib\\site-packages (from openai>=1.67->semantic-kernel==1.34.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\python310\\lib\\site-packages (from openai>=1.67->semantic-kernel==1.34.0) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\python310\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.67->semantic-kernel==1.34.0) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.67->semantic-kernel==1.34.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.67->semantic-kernel==1.34.0) (0.16.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\python310\\lib\\site-packages (from ruamel.yaml>=0.17.10->prance<25.4.9,>=23.6.21->semantic-kernel==1.34.0) (0.2.12)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from tqdm>4->openai>=1.67->semantic-kernel==1.34.0) (0.4.6)\n",
      "Building wheels for collected packages: semantic-kernel\n",
      "  Building wheel for semantic-kernel (pyproject.toml): started\n",
      "  Building wheel for semantic-kernel (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for semantic-kernel: filename=semantic_kernel-1.34.0-py3-none-any.whl size=873082 sha256=6b9f3eaa7c40bf0348929e5b2a47389d8cc01e94dd419da5bb127a12761e9ac5\n",
      "  Stored in directory: C:\\Users\\demouser\\AppData\\Local\\Temp\\2\\pip-ephem-wheel-cache-i6nnx24x\\wheels\\27\\82\\a9\\103dfce4f32d7e3ca9d33130ecff1ea156c949b0105a27fe80\n",
      "Successfully built semantic-kernel\n",
      "Installing collected packages: semantic-kernel\n",
      "Successfully installed semantic-kernel-1.34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/microsoft/semantic-kernel.git 'C:\\Users\\demouser\\AppData\\Local\\Temp\\2\\pip-req-build-mqmzpxdt'\n"
     ]
    }
   ],
   "source": [
    "# ✅ Uninstall old version and install latest Semantic Kernel with PostgresMemoryStore support\n",
    "!pip uninstall -y semantic-kernel\n",
    "!pip install git+https://github.com/microsoft/semantic-kernel.git@main#subdirectory=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ba93fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from psycopg_pool import ConnectionPool\n",
    "from psycopg.sql import SQL, Identifier\n",
    "from semantic_kernel.memory.memory_store_base import MemoryStoreBase\n",
    "from semantic_kernel.memory.memory_record import MemoryRecord\n",
    "from semantic_kernel.exceptions.memory_connector_exceptions import MemoryConnectorInitializationError\n",
    "from semantic_kernel.connectors.postgres import PostgresSettings, DEFAULT_SCHEMA\n",
    "\n",
    "\n",
    "class PostgresMemoryStore(MemoryStoreBase):\n",
    "    def __init__(self, connection_string: str, default_dimensionality: int, schema: str = DEFAULT_SCHEMA):\n",
    "        try:\n",
    "            settings = PostgresSettings(connection_string=connection_string)\n",
    "        except Exception as ex:\n",
    "            raise MemoryConnectorInitializationError(\"Failed to create Postgres settings.\", ex) from ex\n",
    "\n",
    "        self._connection_pool = ConnectionPool(\n",
    "            open=True, kwargs=settings.get_connection_args()\n",
    "        )\n",
    "        self._schema = schema\n",
    "        self._default_dimensionality = default_dimensionality\n",
    "\n",
    "    async def create_collection(self, collection_name: str, dimension_num: int = None):\n",
    "        dim = dimension_num or self._default_dimensionality\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {scm}.{tbl} (\n",
    "                    key TEXT PRIMARY KEY,\n",
    "                    embedding vector({dim}),\n",
    "                    metadata JSONB,\n",
    "                    timestamp TIMESTAMP\n",
    "                )\n",
    "            \"\"\").format(scm=Identifier(self._schema), tbl=Identifier(collection_name), dim=dim))\n",
    "\n",
    "    async def does_collection_exist(self, collection_name: str) -> bool:\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT EXISTS (\n",
    "                    SELECT FROM information_schema.tables \n",
    "                    WHERE table_schema = %s AND table_name = %s\n",
    "                )\n",
    "            \"\"\", (self._schema, collection_name))\n",
    "            return cur.fetchone()[0]\n",
    "\n",
    "    async def get_collections(self) -> list[str]:\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT table_name FROM information_schema.tables\n",
    "                WHERE table_schema = %s\n",
    "            \"\"\", (self._schema,))\n",
    "            return [row[0] for row in cur.fetchall()]\n",
    "\n",
    "    async def get(self, collection_name: str, key: str) -> MemoryRecord | None:\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(\"\"\"\n",
    "                SELECT key, embedding, metadata, timestamp FROM {scm}.{tbl}\n",
    "                WHERE key = %s\n",
    "            \"\"\").format(scm=Identifier(self._schema), tbl=Identifier(collection_name)), (key,))\n",
    "            row = cur.fetchone()\n",
    "            if not row:\n",
    "                return None\n",
    "            meta = row[2]\n",
    "            return MemoryRecord.local_record(\n",
    "                id=row[0],\n",
    "                embedding=np.array(row[1]),\n",
    "                text=meta[\"text\"],\n",
    "                description=meta[\"description\"],\n",
    "                additional_metadata=meta.get(\"additional_metadata\"),\n",
    "                timestamp=row[3]\n",
    "            )\n",
    "\n",
    "    async def get_batch(self, collection_name: str, keys: list[str]) -> list[MemoryRecord]:\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(f\"\"\"\n",
    "                SELECT key, embedding, metadata, timestamp FROM {self._schema}.{collection_name}\n",
    "                WHERE key = ANY(%s)\n",
    "            \"\"\"), (keys,))\n",
    "            rows = cur.fetchall()\n",
    "            return [\n",
    "                MemoryRecord.local_record(\n",
    "                    id=row[0],\n",
    "                    embedding=np.array(row[1]),\n",
    "                    text=row[2][\"text\"],\n",
    "                    description=row[2][\"description\"],\n",
    "                    additional_metadata=row[2].get(\"additional_metadata\"),\n",
    "                    timestamp=row[3]\n",
    "                )\n",
    "                for row in rows\n",
    "            ]\n",
    "\n",
    "    async def upsert(self, collection_name: str, record: MemoryRecord) -> str:\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(\"\"\"\n",
    "                INSERT INTO {scm}.{tbl} (key, embedding, metadata, timestamp)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (key) DO UPDATE\n",
    "                SET embedding = EXCLUDED.embedding,\n",
    "                    metadata = EXCLUDED.metadata,\n",
    "                    timestamp = EXCLUDED.timestamp\n",
    "                RETURNING key\n",
    "            \"\"\").format(scm=Identifier(self._schema), tbl=Identifier(collection_name)),\n",
    "            (\n",
    "                record._id,\n",
    "                record.embedding.tolist(),\n",
    "                json.dumps({\n",
    "                    \"text\": record._text,\n",
    "                    \"description\": record._description,\n",
    "                    \"additional_metadata\": record._additional_metadata\n",
    "                }),\n",
    "                record._timestamp,\n",
    "            ))\n",
    "            return cur.fetchone()[0]\n",
    "\n",
    "    async def upsert_batch(self, collection_name: str, records: list[MemoryRecord]) -> list[str]:\n",
    "        return [await self.upsert(collection_name, r) for r in records]\n",
    "\n",
    "    async def remove(self, collection_name: str, key: str):\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(\"\"\"\n",
    "                DELETE FROM {scm}.{tbl}\n",
    "                WHERE key = %s\n",
    "            \"\"\").format(scm=Identifier(self._schema), tbl=Identifier(collection_name)), (key,))\n",
    "\n",
    "    async def remove_batch(self, collection_name: str, keys: list[str]):\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(f\"\"\"\n",
    "                DELETE FROM {self._schema}.{collection_name}\n",
    "                WHERE key = ANY(%s)\n",
    "            \"\"\"), (keys,))\n",
    "\n",
    "    async def delete_collection(self, collection_name: str):\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(\"DROP TABLE IF EXISTS {scm}.{tbl}\")\n",
    "                        .format(scm=Identifier(self._schema), tbl=Identifier(collection_name)))\n",
    "\n",
    "    async def get_nearest_matches(self, collection_name: str, embedding: np.ndarray, limit: int, min_relevance_score: float = 0.0, with_embeddings: bool = False):\n",
    "        with self._connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "            cur.execute(SQL(f\"\"\"\n",
    "                SELECT key, embedding, metadata, 1 - (embedding <=> %s::vector) AS score, timestamp\n",
    "                FROM {self._schema}.{collection_name}\n",
    "                ORDER BY score DESC\n",
    "                LIMIT {limit}\n",
    "            \"\"\"), (embedding.tolist(),))\n",
    "            rows = cur.fetchall()\n",
    "            return [\n",
    "                (MemoryRecord.local_record(\n",
    "                    id=row[0],\n",
    "                    embedding=np.array(row[1]) if with_embeddings else np.array([]),\n",
    "                    text=row[2][\"text\"],\n",
    "                    description=row[2][\"description\"],\n",
    "                    additional_metadata=row[2].get(\"additional_metadata\"),\n",
    "                    timestamp=row[4],\n",
    "                ), row[3]) for row in rows\n",
    "            ]\n",
    "\n",
    "    async def get_nearest_match(self, collection_name: str, embedding: np.ndarray, min_relevance_score: float = 0.0, with_embedding: bool = False):\n",
    "        matches = await self.get_nearest_matches(collection_name, embedding, limit=1, min_relevance_score=min_relevance_score, with_embeddings=with_embedding)\n",
    "        return matches[0] if matches else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "40f914df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\demouser\\AppData\\Local\\Temp\\2\\ipykernel_5884\\2105210955.py:44: DeprecationWarning: This class will be removed in a future version.\n",
      "  semantic_memory = SemanticTextMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// Functions the Agent Called: //\n",
      "search_graph_cases was called\n",
      "count_cases was called\n",
      "// Prompt with Added Memory Context: //\n",
      "Here are things we’ve discussed before:\n",
      "- Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above. High accuracy is important, and high number of citations is important. Also how many cases are there overall?\n",
      "- Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above. High accuracy is important, and high number of citations is important. Also how many cases are there overall?\n",
      "- Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above. High accuracy is important, and high number of citations is important. Also how many cases are there overall?\n",
      "\n",
      "Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above. High accuracy is important, and high number of citations is important. Also how many cases are there overall?\n",
      "// Agent Response: //\n",
      "### Total Number of Cases:\n",
      "There are a total of 118,6056 cases overall in the database.\n",
      "\n",
      "### Highly Relevant Cases Related to Water Leakage in Apartments:\n",
      "Here are 10 relevant cases:\n",
      "\n",
      "1. **Stuart v. Coldwell Banker Commercial Group, Inc.**  \n",
      "   **Reasoning:** Key case addressing construction defects and damages awarded to homeowners associations, including issues related to common areas.  \n",
      "   **Summary:** Overturns trial decision improperly applying the discovery rule and limits implied warranty of habitability.\n",
      "\n",
      "2. **Wilber Development Corp. v. Les Rowland Constr., Inc.**  \n",
      "   **Reasoning:** Addresses water mismanagement by storm drainage constructed by land developers, creating property damage.  \n",
      "   **Summary:** Court dismisses plaintiff's inverse condemnation action for water drainage impacts.\n",
      "\n",
      "3. **Wood v. City of Tacoma**  \n",
      "   **Reasoning:** Case discusses damages from city drainage and street grading affecting private property watersheds.  \n",
      "   **Summary:** Dismisses claims of property damage caused by drainage system development.\n",
      "\n",
      "4. **Bach v. Sarich**  \n",
      "   **Reasoning:** While not directly water leakage, zoning and construction issues provide context for alterations causing damages.  \n",
      "   **Summary:** Enjoins construction affecting natural recreational spaces.\n",
      "\n",
      "5. **Foisy v. Wyman**  \n",
      "   **Reasoning:** Involves unpaid rent and damages relating to property management issues.  \n",
      "   **Summary:** Upholds the need for compliance in lease agreements.\n",
      "\n",
      "6. **Schedler v. Wagner**  \n",
      "   **Reasoning:** Discusses liability for unsafe conditions in apartment buildings.  \n",
      "   **Summary:** Awards damages for injury after tenant fell on icy sidewalk.\n",
      "\n",
      "7. **Laurelon Terrace, Inc. v. City of Seattle**  \n",
      "   **Reasoning:** Explores flooding and negligence in city management affecting private property.  \n",
      "   **Summary:** Grants new trial on instructions regarding contributory negligence.\n",
      "\n",
      "8. **Papac v. City of Montesano**  \n",
      "   **Reasoning:** Addresses consequences of city construction projects causing flooding in citizen homes.  \n",
      "   **Summary:** Awards damages against the city for nuisance causing home damage.\n",
      "\n",
      "9. **Trigg v. Timmerman**  \n",
      "   **Reasoning:** Relating to disputes over drainage ditches between private land.  \n",
      "   **Summary:** Rejects plaintiff’s request to restrain water drainage from neighbor's land.\n",
      "\n",
      "10. **DeHoney v. Gjarde**  \n",
      "   **Reasoning:** Issues involving faulty construction causing water-related damage after completion of work.  \n",
      "   **Summary:** Fault identified during wet seasons despite successful initial construction.\n",
      "\n",
      "These cases provide insights into arguments related to construction defects, drainage mismanagement, and landlord liability. Let me know if you need further explanations!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import uuid\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding import AzureTextEmbedding\n",
    "from semantic_kernel.memory.memory_store_base import MemoryStoreBase\n",
    "\n",
    "# 👇 Register your custom memory store if not already registered\n",
    "if not issubclass(PostgresMemoryStore, MemoryStoreBase):\n",
    "    MemoryStoreBase.register(PostgresMemoryStore)\n",
    "\n",
    "# === ✅ STEP 1: Define the user query ===\n",
    "user_query = (\n",
    "    \"Help me find 10 highly relevant cases related to water leaking in my personal home apartment from the floor above. \"\n",
    "    \"High accuracy is important, and high number of citations is important. Also how many cases are there overall?\"\n",
    ")\n",
    "\n",
    "# === ✅ STEP 2: PostgreSQL connection string ===\n",
    "conn_str = (\n",
    "    f\"host={DB_CONFIG['host']} \"\n",
    "    f\"port={DB_CONFIG['port']} \"\n",
    "    f\"dbname={DB_CONFIG['dbname']} \"\n",
    "    f\"user={DB_CONFIG['user']} \"\n",
    "    f\"password={DB_CONFIG['password']} \"\n",
    "    f\"sslmode={DB_CONFIG['sslmode']}\"\n",
    ")\n",
    "\n",
    "# === ✅ STEP 3: Initialize Postgres-backed memory store ===\n",
    "memory_store = PostgresMemoryStore(\n",
    "    connection_string=conn_str,\n",
    "    default_dimensionality=1536\n",
    ")\n",
    "\n",
    "# === ✅ STEP 3.5: Create collection if needed (first time setup)\n",
    "await memory_store.create_collection(\"agent_memories\")\n",
    "\n",
    "# === ✅ STEP 4: Azure embedding model for memory ===\n",
    "embedding_generator = AzureTextEmbedding(\n",
    "    deployment_name=\"text-embedding-3-small\",\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_KEY\n",
    ")\n",
    "\n",
    "# === ✅ STEP 5: Create the memory interface ===\n",
    "semantic_memory = SemanticTextMemory(\n",
    "    storage=memory_store,\n",
    "    embeddings_generator=embedding_generator\n",
    ")\n",
    "\n",
    "# === ✅ STEP 6: Save the new query to memory ===\n",
    "await semantic_memory.save_information(\n",
    "    collection=\"agent_memories\",\n",
    "    text=user_query,\n",
    "    id=str(uuid.uuid4()),\n",
    "    description=\"User query\"\n",
    ")\n",
    "\n",
    "# === ✅ STEP 7: Search for similar past queries ===\n",
    "recalls = await semantic_memory.search(\n",
    "    collection=\"agent_memories\",\n",
    "    query=user_query,\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "# === ✅ STEP 8: Format the recalled memory context ===\n",
    "memory_context = \"\\n\".join(f\"- {m.text}\" for m in recalls)\n",
    "\n",
    "# === ✅ STEP 9: Build the agent prompt ===\n",
    "prompt = (\n",
    "    f\"Here are things we’ve discussed before:\\n{memory_context}\\n\\n\"\n",
    "    f\"{user_query}\"\n",
    ")\n",
    "\n",
    "# === ✅ STEP 10: Query the agent with memory-enhanced prompt ===\n",
    "print(\"// Functions the Agent Called: //\")\n",
    "response = await agent.get_response(messages=prompt)\n",
    "\n",
    "# === ✅ STEP 11: Print prompt and response ===\n",
    "print(\"// Prompt with Added Memory Context: //\")\n",
    "print(prompt)\n",
    "print(\"// Agent Response: //\")\n",
    "print(response.content)\n",
    "\n",
    "# === ✅ STEP 12: Save the agent's reply back to memory ===\n",
    "await semantic_memory.save_information(\n",
    "    collection=\"agent_memories\",\n",
    "    text=str(response.content),\n",
    "    id=str(uuid.uuid4()),\n",
    "    description=\"Agent reply\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d1a23",
   "metadata": {},
   "source": [
    "## Congratulations you have completed the Lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9dba0",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We have curated additional resources to enhance your ongoing journey in building AI agents and AI-powered applications with Azure Database for PostgreSQL.\n",
    "\n",
    "- A more detailed blog post about the legal case example of lab in the [GraphRAG Solution for Azure Database for PostgreSQL](https://aka.ms/pg-graphrag) check the code in the [GitHub repository](https://aka.ms/postgres-graphrag-solution).\n",
    "- Learn more about [Graph data in Azure Database for PostgreSQL](https://aka.ms/age-blog).\n",
    "- Get familiar with the new [PostgreSQL extension for Visual Studio Code]().\n",
    "- Learn more about Semantic Ranking with the [Semantic Ranker Solution Accelerator](https://aka.ms/semantic-ranker-solution-accelerator-pg-blog) and its associated [GitHub repository](https://aka.ms/pg-ranker-repo).\n",
    "- Finally, our more recent solutions accelerator [Build your own advanced AI Copilot with Postgres](http://aka.ms/pg-byoac-docs) teaches you how to extract data from statements of work (SOWs) and invoices in PDF files and use AI to validate them, more details in the [GitHub repo](http://aka.ms/pg-byoac-repo)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
